# Linear models

## What is a linear model?

### Defining mean and variation

Scientists have always been interested in determining relationships
between variables. In this workshop we will learn how to use linear
models, a set of models that quantify relationships between variables.

To begin, we will define some important concepts that are central to
linear models: mean and variation. Mean is a measure of the average
value of a population. Suppose we have a random variable x, for example
the height of the people in this room, and we would like to see some
patterns of this variable. The first way we will present it will be by
using the mean (be aware that there are many ways of measuring it):

But the mean alone will not represent it a population fully. We can also
describe the population using measures of variation. Variation is the
spread around the mean. For example, whether all people in the room are
approximately the same height (low variation) or if there are many tall
and short people (high variation). Mean deviation, variance, standard
deviation and the coefficient of deviation are all measures of variation
which we will define below. We can measure deviation of each element
from the mean:

With the deviation for each value, we can calculate the mean deviation:

To convert all variables to positive values without the need of absolute
values, we can also square the value. And that is where the variance
comes from:

However, by squaring each value, our variables are no longer in
meaningful units. Back to our example with the height of people in this
room, the variance will be \$ mˆ2 \$ which is not what we're measuring.
To convert the value to meaningful units, we can calculate the standard
deviation:

Finally, to express the coefficient of variation, known as the relative
standard deviation, expressed in percentage, we have:

### Linear models

In linear models, we use the concepts of mean and variation to describe
the relationship between two variables. Linear models are so named
because they describe the relationship between variables as lines:

$ {y_i} = {β_0} + {β_1}{x\_{i1}} + \... + {β_p}{x\_{ip}} + {ε_i}
\</m\>

where\
${y_i}$is the response variable,\
${β_0}$is the intercept of the regression line,\
${β_1}$is the coefficient of variation for the first
explanatory variable,\
${β_p}$is the coefficient of variation for the pth explanatory
variable,\
${x_i1}$is the first explanatory variable,\
${x_ip}$is the pth explanatory variable,\
${ε_i}$are the residuals of the model\
The response variable is the variable you want to explain. It's also
known as the dependent variable. There is only one response variable.
The explanatory variables are the variables you think may explain your
response variable. They're also known as independent variables. There
can be one or many explanatory variables. For example, suppose we want
to explain variation in the height of people in a room. Height is the
response variable. Some possible explanatory variables could be gender
or age.

In linear models the response variable must be continuous, while the
explanatory variables can be continuous or categorical. A continuous
variable has an infinite number of possible values. A categorical
variable has a limited number of possible values. Age, temperature, and
latitude are all continuous variables. Sex, developmental stage, and
country are all categorical variables. For continuous explanatory
variables, the linear model tests whether there is a significant
correlation between the explanatory and response variable. For
categorical explanatory variables, the linear model tests whether there
is a significant difference between the different levels (groups) in
their mean value of the response variable. This should become clearer as
we learn about specific types of linear models in the sections below.

In almost all cases, the explanatory variables will not explain all of
the variation in the response variable. Gender and age, for example,
will not be enough to predict everyone's height perfectly. The
remaining, unexplained variation is called error or residuals.

The goal of the linear model is to find the best estimation of the
parameters (the β variables) and then assess the goodness of fit of the
model. Several methods have been developed to calculate the intercept
and coefficients of linear models, and the appropriate choice depends on
the model. The general concept behind these methods is that the
residuals are minimized.

Depending on the kind of explanatory variables considered and their
number, different statistical tools can be used to assess these
relationships. The table below lists the five types of statistical
analysis that will be covered in this workshop:

  ----------------------------------------------------------------------------------------------------------------
  Statistical analysis       Type of response\   Type of explanatory\   Number of\                    Number of\
                             variable Y          variable X             explanatory variables         levels k
  -------------------------- ------------------- ---------------------- ----------------------------- ------------
  Simple linear regression   Continuous          Continuous             1

  t-test                     :::                 Categorical            1                             2

  ANOVA                      :::                 Categorical            1 (one-way ANOVA), 2 (two-\   3 or more
                                                                        way ANOVA) or more

  ANCOVA                     :::                 Continuous\            2 or more                     2 or more
                                                 AND categorical

  Multiple regression        :::                 Continuous             2 or more
  ----------------------------------------------------------------------------------------------------------------

### Linear model assumptions

To be valid, a linear models must meet 4 assumptions, otherwise the
model results cannot be safely interpreted.

1.  The residuals are independent
2.  The residuals are normally distributed
3.  The residuals have a mean of 0
4.  The residuals are homoskedastic (they have constant variance)

Note that all of these assumptions concern the residuals, not the
response or explanatory variables. The residuals must be independent,
meaning that there isn't an underlying structure missing from the model
(usually spatial or temporal autocorrelation). The residuals must be
normally distributed with a mean of 0, meaning that the largest
proportion of residuals have a value close to 0 (ie, the error is small)
and the distribution is symmetrical (ie, the response variable is
overestimated and underestimated equally often). The residuals must be
homoskedastic, meaning that error doesn't change much as the value of
the predictor variables change.

In the following sections, we do not always explicitly restate the above
assumptions for every model. Be aware, however, that these assumption
are implicit in all linear models, including all models presented below.

### Test statistics and p-values

Once you've run your model in R, you will receive a model output that
includes many numbers. It takes practice to understand what each of
these numbers means and which to pay the most attention to. The model
output includes the estimation of the parameters (the β variables). The
output also includes test statistics. The particular test statistic
depends on the linear model you are using (t is the test statistic for
the linear regression and the t test, and F is the test statistic for
ANOVA).

In linear models, the null hypothesis is typically that there is no
relationship between two continuous variables, or that there is no
difference in the levels of a categorical variable. The larger the
absolute value of the test statistic, the more improbable that the null
hypothesis is true. The exact probability is given in the model output
and is called the p-value. You could think of the p-value as the
probability that the null hypothesis is true, although that's a bit of
a simplification. (Technically, the p-value is the probability that,
given the assumption that the null hypothesis is true, the test
statistic would be the same as or of greater magnitude than the actual
observed test statistic.) By convention, we consider that if the p value
is less than 0.05 (5%), then we reject the null hypothesis. This cut-off
value is called α (alpha). If we reject the null hypothesis then we say
that the alternative hypothesis is supported: there is a significant
relationship or a significant difference. Note that we do not \"prove\"
hypotheses, only support or reject them.