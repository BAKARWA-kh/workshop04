# Linear regression in R

## Simple linear regression

Simple linear regression is a type of linear model which contains a
single, continuous explanatory variable. The regression tests whether
there is a significant correlation between the two variables.

Simple linear regression involves two parameters which must be
estimated: an intercept (${β_0}\</m\>) and a coefficient of
correlation (${β_1}\</m\>). Ordinary least squares is the most
widely used estimation method, and also corresponds to the default
method of the `lm` function in R. Ordinary least squares fits a line
such that the sum of the squared vertical distances between the observed
data and the linear regression model (i.e. the residuals) is minimized.

Click below to see the math in more detail.  Using ordinary
least squares, the intercept β~0~ and the slope β~1~ of the linear
regression can be calculated as:

$β\_{1}={sum{i}{}{(x\_{i}y\_{i})}-overline{x}overline{y}}/sum{i}{}{(x\_{i}-overline{x})}\^2
= {Cov(x,y)}/{Var(x)}\</m\>

$β\_{0}=overline{y}-β\_{1}overline{x}$

## Work flow

Below we will explore several kinds of linear models. The way you create
and interpret each model will differ in the specifics, but the
principles behind them and the general work flow will remain the same.
For each model we will work through the following steps:

1.  Visualize the data (data visualization could also come later in your
    work flow)
2.  Create a model
3.  Test the model assumptions
4.  Adjust the model if assumptions are violated
5.  Interpret the model results

### 2.1 Running a linear model

Using the bird dataset, we will first examine the linear regression of
maximum abundance as a function of mass.

In R, linear regression is implemented using the lm function from the
stats package:

`lm (y ~ x)`

  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [**Note**]{.ul}: before using a new function in R, users should refer to its help documentation (`?functionname`) to find out how to use the function as well as its preset default methods.
  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

```{r, echo = TRUE, eval = FALSE}
# Loading libraries and bird dataset
library(e1071)
library(MASS)
setwd("~/Desktop/...") # Don't forget to set your working directory (note: your directory will be different)
bird<-read.csv("birdsdiet.csv")

# Visualize the dataframe
names(bird)
str(bird)
head(bird)
summary(bird)
plot(bird)
```

The bird dataset contains 7 variables:

  --------------------------------------------------------------------------------------------------------------------------
  Variable Name   Description                                                   Type
  --------------- ------------------------------------------------------------- --------------------------------------------
  Family          Common name of family                                         String

  MaxAbund        The highest observed abundance at any site\                   Continuous/ numeric
                  in North America

  AvgAbund        The average abundance across all sites\                       Continuous/ numeric
                  where found in NA

  Mass            The body size in grams                                        Continuous/ numeric

  Diet            Type of food consumed                                         Discrete -- 5 levels (Plant; PlantInsect;\
                                                                                Insect; InsectVert; Vertebrate)

  Passerine       Is it a songbird/ perching bird                               Boolean (0/1)

  Aquatic         Is it a bird that primarily lives in/ on/ next to the water   Boolean (0/1)
  --------------------------------------------------------------------------------------------------------------------------

Note that Family, Diet, Passerine, and Aquatic are all categorical
variables although they are encoded in different ways (string, discrete,
boolean).

We are now ready to run our linear model:

```{r, echo = TRUE, eval = FALSE}
lm1 <- lm(bird$MaxAbund ~ bird$Mass) # where Y ~ X means Y "as a function of" X>
```

### Verifying assumptions

```{r, echo = TRUE, eval = FALSE}
opar <- par(mfrow=c(2,2)) # draws subsequent figures in a 2-by-2 panel
plot(lm1)
par(opar) # resets to 1-by-1 plot
```

#### Verifying independence

Linear models can only be applied to independent data. This means that
the yi at a given xi value must not be influenced by other xi values.
Violation of independence can happen if your data represent some form of
dependence structure, such as spatial or temporal correlation.

There is no simple diagnostic plot for independence, unfortunately.
Instead, you must consider your data carefully. Is there some underlying
structure in your data that makes your data points dependent on each
other? If you collect data from the same sites over time (ie, a time
series) or if you collect multiple data points from the same organism,
your data violates the assumption of independence. You will need to use
a different type of model instead.

#### Verifying residual variance is constant and residual mean is 0

*Residual vs Fitted plot* - The first graph of the diagnostic plots is
called by `plot(lm1)`. This plot illustrates the spread of the residuals
between each fitted values. Each point represents the distance of the
response variable from the model prediction of the response variable. If
the residuals spread randomly around the 0 line, this indicates that the
relationship is linear and that the mean of the residuals is 0. If the
residuals form an approximate horizontal band around the 0 line, this
indicates homogeneity of error variance (ie, it is homoskedastic). If
the residuals form a funnel shape, this indicates the residuals are not
homoskedastic.

![](images/workshop_3_lm1_residuals_vs_fitted.png){width="300"}

*Scale-location plot* - The third graph of the diagnostic plots enables
one to verify whether the residual spread increases with a given fitted
values (i.e. identifies whether the spread in the residuals is due to
the selected explanatory variable). If the spread increases, the
homoscedasticity assumption is not respected.

![](images/workshop_3_lm1_scale-location.png){width="300"}

#### Verifying that residuals are normally distributed

*QQ plot* - Normality can be assessed from the QQplot of the diagnostic
plots. This graph compares the probability distribution of the model
residuals to the probability distribution of normal data series. If the
standardized residuals lie linearly on the 1:1 line of the QQplot, the
residuals can be considered normally distributed.

![](images/workshop_3_lm1_qq.png){width="300"}

The points of the QQplot are nonlinear, which suggests that the
residuals are not normally distributed.

#### Checking for high leverage

In addition to the assumption testing above, we are also interested in
whether any of our data points have high leverage. This is not
assumption testing *per se*, but it will affect our interpretation of
the data. If some of the observations in a dataset possess strongly
different values from others, a model fitting problem can arise such
that these high leverage data influence the model calculation.

*Residuals vs Leverage plot* - High leverage data can be visualised on
the fourth diagnostic plots (i.e. residuals vs leverage), which
identifies the observation numbers of the high leverage data point(s).
If (and only if!) these observations correspond to mis-measurements or
represent exceptions, they can be removed from the original dataset.\
![](images/workshop_3_lm1_leverage.png){width="300"}

### Normalizing data

In the example provided above, the model residuals were not normally
distributed and therefore the assumption of residual normality is
violated. We may still be able to use a linear regression model if we
can address this violation. The next step is to try to normalize the
variables using transformations. Often if we can make the explanatory
and/or response variables normally distributed then the model residuals
will become normally distributed. In addition to QQ-plots we can assess
the normality of a variable by drawing a histogram using the function
`hist`, and check visually whether the data series appears to follow a
normal distribution. For example:

```{r, echo = TRUE, eval = FALSE}
# Plot Y ~ X and the regression line
plot(bird$MaxAbund ~ bird$Mass, pch=19, col="coral", ylab="Maximum Abundance",
     xlab="Mass")
abline(lm1, lwd=2)
?plot # For further details on plot() arguments
# see colours() for list of colours

# Is the data normally distributed?
hist(bird$MaxAbund,col="coral", main="Untransformed data",
     xlab="Maximum Abundance")
hist(bird$Mass, col="coral", main="Untransformed data", xlab="Mass")
```

![](images/lm1_yvsx.png){width="300"} ![](images/maxabund_hist.png){width="300"}
![](images/mass_hist.png){width="300"}

A third way to assess normality is to use the Shapiro-Wilk normality
test that compares the distribution of the observed data series to a
normal distribution using the function `shapiro.test`.

The null and alternate hypotheses of this test are:

H~0~: the observed data series is normally distributed,\
H~1~: the observed data series is not normally distributed,

The observed data series can be considered normally distributed when the
p-value calculated by the Shapiro-Wilk normality test is greater than or
equal to α, typically set to 0.05.

```{r, echo = TRUE, eval = FALSE}
# Test null hypothesis that the sample came from a normally distributed population
shapiro.test(bird$MaxAbund)
shapiro.test(bird$Mass)
# If p < 0.05, then distribution is not-normal
# if p > 0.05, then distribution is normal
```

We can also evaluate the skewness of each distribution using the
`Skewness` function:

```{r, echo = TRUE, eval = FALSE}
skewness(bird$MaxAbund)
skewness(bird$Mass)
# where positive values indicate a left-skewed distribution, and negative value a right skew.
```

The histograms, Shapiro tests and Skewness all indicate that the
variables need to be transformed to normalize (e.g. a log~10~
transformation).

### Data transformation

In case of non-normality, response and explanatory variables can be
transformed to enhance their normality following these rules:

  --------------------------------------------------------------------------------------------------
  Type of distribution              Transformation             R function
  --------------------------------- -------------------------- -------------------------------------
  Moderately positive skewness      $sqrt{x}$        sqrt(x)

  Substantially positive skewness   $log_10{(x)}$    log10(x)

  Substantially positive skewness   $log_10{(x+C)}$  log10(x + C) where C is a constant\
                                                               added to each value of x so that\
                                                               the smallest score is 1

  Moderately negative skewness      $sqrt{(K-x)}$    sqrt(K - x) where K is a constant\
                                                               subtracted from each value of x\
                                                               so that the smallest score is 1

  Substantially negative skewness   $log_10{(K-x)}$  log10(K - x)
  --------------------------------------------------------------------------------------------------

Thus, log~10~ transformations should be applied and saved in the bird
data frame. The model can then be re-run, verified and interpreted.

```{r, echo = TRUE, eval = FALSE}
# Add log10() transformed variables to your dataframe
bird$logMaxAbund <- log10(bird$MaxAbund)
bird$logMass <- log10(bird$Mass)
names(bird) # to view the dataframe + new transformed variables

hist(bird$logMaxAbund,col="yellowgreen", main="Log transformed",
     xlab=expression("log"[10]*"(Maximum Abundance)"))
hist(bird$logMass,col="yellowgreen", main="Log transformed",
     xlab=expression("log"[10]*"(Mass)"))
shapiro.test(bird$logMaxAbund); skewness(bird$logMaxAbund)
shapiro.test(bird$logMass); skewness(bird$logMass)

# Re-run your analysis with the appropriate transformations
lm2 <- lm(bird$logMaxAbund ~ bird$logMass)

# Are there remaining problems with the diagnostics (heteroscedasticity, non-independence, high leverage)?
opar <- par(mfrow=c(2,2))
plot(lm2, pch=19, col="gray")
par(opar)
```

![](images/hist_logmaxabund.png){width="300"}
![](images/hist_logmass.png){width="300"} ![](images/plot_lm2_.png){width="550"}

### Model output

Once all these assumptions have been verified, the model results can be
interpreted. These results are called in R using the function `summary`.

```{r, echo = TRUE, eval = FALSE}
# Now we can look at the model coefficients and p-values
summary(lm2)

# You can also just call up the coefficients of the model
lm2$coef

# What else?
str(summary(lm2))
summary(lm2)$coefficients # where Std. Error is the standard error of each estimate
summary(lm2)$r.squared # Coefficient of determination
summary(lm2)$adj.r.squared # Adjusted coefficient of determination
summary(lm2)$sigma # Residual standard error (square root of Error Mean Square)
# etc…

# You can also check for yourself the equation for R2:
SSE = sum(resid(lm2)^2)
SST = sum((bird$logMaxAbund - mean(bird$logMaxAbund))^2)
R2 = 1 - ((SSE)/SST)
R2
```

The output of this function presents all the results of your validated
model:

    lm(formula = logMaxAbund ~ logMass, data = bird)
    Residuals:
              Min         1Q          Median          3Q          Max
                   -1.93562   -0.39982    0.05487     0.40625     1.61469
                  Estimate    Std. Error  t value     Pr(>|t|)
    (Intercept)     1.6724        0.2472      6.767       1.17e-08 ***
    logMass          -0.2361          0.1170      -2.019      0.0487 *
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
    Residual standard error: 0.6959 on 52 degrees of freedom
    Multiple R-squared:  0.07267,   Adjusted R-squared:  0.05484
    F-statistic: 4.075 on 1 and 52 DF,  p-value: 0.04869

The coefficients of the regression model and their associated standard
error appear in the second and third columns of the regression table,
respectively. Thus,\
β~0~ = 1.6724 ± 0.2472 is the intercept (± se) of the regression model,\
β~1~ = -0.2361 ± 0.1170 is the slope (± se) of the regression model.

and finally: $ $*logMaxAbund* = 1.6724 (± 0.2472) - 0.2361 (±
0.1170) x *logMass*

The t-value and their associated p-value (in the fourth and fifth
columns of the regression table, respectively) test for a significant
difference between the calculated coefficients and zero. In this case,
we can see that *logMass* has a significant influence on *logMaxAbund*
because the p-value associated with the slope of the regression model is
inferior to 0.05. Moreover, these two variables are negatively related
as the slope of the regression model is negative.\
Users must, however, be aware that significant relationship between two
variables does not always imply causality. Conversely, the absence of
significant linear regression between y and x does not always imply an
absence of relationship between these two variables; this is for example
the case when a relationship is not linear.

The goodness of fit of the linear regression model is assessed from the
**adjusted-R^2^** (here, 0.05484). This value is a measure of the
proportion of variation explained by the model.

Click below to see the math in more detail.  $
overline{R}\^2=1-(1-R\^2){n-1}/{n-p-1} \</m\>

where\
p is the total number of regressors and n is the sample size,\
$ R\^2={SS_reg}/{SS_tot} \</m\>\
$ {SS_tot}=sum{i}{}{({y_i}-overline{y})}\^2 $is the total sums
of squares,\
$ {SS_reg}=sum{i}{}{(hat{y_i}-overline{y})}\^2 $is the
regression sums of squares - also called the explained sums of squares.



The higher the adjusted-R^2^ is, the better the data fit the statistical
model, knowing that this coefficient varies between 0 and 1. In this
case, the relationship between *logMaxAbund* and *logMass* is quite
weak.\
The last line of the R output represents the F-statistic of the model
and its associated p-value. If this p-value is inferior to 0.05, the
model explains the data relationship better than a null model.\
==== 2.6 Plotting ====

Linear regression results are generally represented by a plot of the
response variable as a function of the explanatory variable on which the
regression line is added (and if needed the confidence intervals), using
the R code:

```{r, echo = TRUE, eval = FALSE}
plot(logMaxAbund ~ logMass, data=bird, pch=19, col="yellowgreen",
                   ylab = expression("log"[10]*"(Maximum Abundance)"), xlab = expression("log"[10]*"(Mass)"))
abline(lm2, lwd=2)

# You may also flag the previously identified high-leveraged points
points(bird$logMass[32], bird$logMaxAbund[32], pch=19, col="violet")
points(bird$logMass[21], bird$logMaxAbund[21], pch=19, col="violet")
points(bird$logMass[50], bird$logMaxAbund[50], pch=19, col="violet")

# We can also plot the confidence intervals
confit<-predict(lm2,interval="confidence")
points(bird$logMass,confit[,2])
points(bird$logMass,confit[,3])
```

![](images/yvxx_lm2.png){width="400"}

### Subsetting

We may also run the analysis on a subset of observations, for example,
on terrestrial birds only.

```{r, echo = TRUE, eval = FALSE}
# Recall that you can exclude objects using "!"
# We can analyze a subset of this data using this subset command in lm()
lm3 <- lm(logMaxAbund ~ logMass, data=bird, subset =! bird$Aquatic) # removing the Aquatic birds

# or equivalently
lm3 <- lm(logMaxAbund ~ logMass, data=bird, subset=bird$Aquatic == 0)

# Examine the model
opar <- par(mfrow=c(2,2))
plot(lm3, pch=19, col=rgb(33,33,33,100,maxColorValue=225))
summary(lm3)
par(opar)

# Compare the two datasets
opar <- par(mfrow=c(1,2))
plot(logMaxAbund ~ logMass, data=bird, main="All birds", ylab = expression("log"[10]*"(Maximum Abundance)"),
     xlab = expression("log"[10]*"(Mass)"))
abline(lm2,lwd=2)

plot(logMaxAbund ~ logMass, data=bird, subset=!bird$Aquatic, main="Terrestrial birds",
     ylab = expression("log"[10]*"(Maximum Abundance)"), xlab = expression("log"[10]*"(Mass)"))
abline(lm3,lwd=2)
opar(par)
```


## CHALLENGE 1

Examine the relationship between *log~10~(MaxAbund)* and *log~10~(Mass)*
for passerine birds.\
`HINT:` Passerine is coded 0 and 1 just like Aquatic. You can verify
this by viewing the structure `str(bird)`.

++++ Challenge 1: Solution \|

```{r, echo = TRUE, eval = FALSE}
lm4 <- lm(logMaxAbund ~ logMass, data=bird, subset=bird$Passerine == 1)

# Examine the diagnostic plots
opar <- par(mfrow=c(2,2))
plot(lm4)
summary(lm4)
par(opar)

# Compare variance explained by lm2, lm3 and lm4
str(summary(lm4)) # Recall: we want adj.r.squared
summary(lm4)$adj.r.squared # R2-adj = -0.02
summary(lm2)$adj.r.squared # R2-adj = 0.05
summary(lm3)$adj.r.squared # R2-adj = 0.25

# Visually compare the three models
opar <- par(mfrow=c(1,3))
plot(logMaxAbund ~ logMass, data=bird, main="All birds",
     ylab = expression("log"[10]*"(Maximum Abundance)"),
     xlab = expression("log"[10]*"(Mass)"), pch=19, col="yellowgreen")
abline(lm2,lwd=2)

plot(logMaxAbund ~ logMass, subset=Passerine == 1, data=bird, main="Passerine birds",
     ylab = expression("log"[10]*"(Maximum Abundance)"),
     xlab = expression("log"[10]*"(Mass)"), pch=19, col="violet")
abline(lm4,lwd=2)

plot(logMaxAbund ~ logMass, data=bird, subset=!bird$Aquatic, main="Terrestrial birds",
     ylab = expression("log"[10]*"(Maximum Abundance)"),
     xlab = expression("log"[10]*"(Mass)"), pch=19, col="skyblue")
abline(lm3,lwd=2)
par(opar)
```

**Conclusion:** The best model among the three is lm3 (only terrestrial
birds) ![](images/lm2_lm3_lm4.png){width="850"} ++++
