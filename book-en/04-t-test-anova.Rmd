# t-test and ANOVA 

## Analysis of Variance (ANOVA)

Analysis of Variance (ANOVA) is a type of linear model for a continuous
response variable and one or more categorical explanatory variables. The
categorical explanatory variables can have any number of levels
(groups). For example, the variable \"colour\" might have three levels:
green, blue, and yellow. ANOVA tests whether the means of the response
variable differ between the levels by comparing the variation within a 
group with the variation amon groups. For example, if blueberries differ
in their mass depending on their colour.

ANOVA calculations are based on the sum of squares partitioning and
compares the within-level variance to the between-level
variance. If the between-level variance is greater than the
within-level variance, this means that the levels affect the
explanatory variable more than the random error (corresponding to the
within-level variance), and that the explanatory variable is likely
to be significantly influenced by the levels.

```{r, echo = FALSE, fig.height=3, fig.width=6.5}
source('images/figAnova.R')
```

In the ANOVA, the comparison of the between-level variance to the
within-level variance is made through the calculation of the
F-statistic that correspond to the ratio of the mean sum of squares of
the level (MS~Lev~) on the mean sum of squares of the error (MS~E~).
These two last terms are obtained by dividing their two respective sums
of squares by their corresponding degrees of freedom, as is typically
presented in a ANOVA table . Finally, the p-value of the ANOVA is calculated 
from the F-statistic that follows a Chi-square (χ^2^) distribution.

  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  Source of\   Degrees of\         Sums of squares                                                  Mean squares                                                 F-statistic
  variation    freedom (df)
                                                           
  ------------ ------------------- -------------------------------------------------------------- ------------------------------------------------------ -------------------------------------- -----------------------------------
  Total        $ra-1$               $SS_{t}=\sum(y_{i}-\overline{y})^{2}$

  Facteur A     $a-1$               $SS_{f}=\sum(\hat{y_{i}}-\overline{y})^{2}$                   $MS_{f}=\frac{SS_{f}}{(a-1)}$                            $F=\frac{MS_{f}} {MS_{E}}$

  Error        $a(r-1)$               $SS_{\epsilon}=\sum(y_{i}-\hat{y_{i}})^{2}$                           ${MS_E}=\frac{SS_{\epsilon}}{a(r-1)}$
  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

$a$: number of levels of the explanatory variable A

$r$: number of replicates per level

$\overline{y}$: general mean of the explanatory variable

$\hat{y_{i}}$ : mean of the explanatory variable for all the replicates of the level i.


### 3.1 Types of ANOVA

1.  **One-way ANOVA**\
    One categorical explanatory variable with 2 or more levels. If there
    are 2 levels a **t-test** can be used alternatively.
2.  **Two-way ANOVA** *[(see section
    below)](http://qcbs.ca/wiki/r_workshop4?&#two-way_anova)*\
    - 2 categorical explanatory variables or more,\
    - Each categorical explanatory variable can have multiple levels,\
    - The interactions between each categorical explanatory variable
    can be tested.
3.  **Repeated measures**\
    ANOVA can be used for repeated measures, but we won't cover this
    today. **Linear Mixed-effect Models** can also be used for this kind
    of data *[(see Workshop 6)](http://qcbs.ca/wiki/r_workshop6)*.

### 3.2 T-test

When you have a single explanatory variable which is qualitative and only have **two levels**, you
can run a *student's T-test* to test for a difference in the mean of the
two levels. If appropriate for your data, you can choose to test a
unilateral hypothesis. This means that you can test the more specific
assumption that one level has a higher mean than the other, rather than
that they simply have different means.Note that robustness of this test increases with sample size and is higher when groups have equal sizes


For the t-test, the t statistic used to find the p-value calculation is calculated as:
$t = (\overline{y_{1}}-\overline{y_{2}})/\sqrt{\frac{s_{1}^2} n_{1} + \frac{s_{2}^2} n_{2}}$

where

$\overline{y_{1}}$ and $\overline{y_{2}}$ are the means of the response variable y for group 1 and 2, respectively,\
$s_{1}^2$ and $s_{2}^2$ are the variances of the response variable y for group 1 and 2, respectively,\
$n_{1}$ and $n_{2}$ are the sample sizes of groups 1 and 2, respectively.


Note that the t-test is mathematically equivalent to a one-way ANOVA
with 2 levels.

#### Assumptions

If the assumptions of the t-test are not met, the test can give
misleading results. Here are some important things to note when testing
the assumptions of a t-test.

1.  **Normality of data**\
    As with simple linear regression, the residuals need to be normally
    distributed. If the data are not normally distributed, but have
    reasonably symmetrical distributions, a mean which is close to the
    centre of the distribution, and only one mode (highest point in the
    frequency histogram) then a t-test will still work as long as the
    sample is sufficiently large (rule of thumb \~30 observations). If
    the data is heavily skewed, then we may need a very large sample
    before a t-test works. In such cases, an alternate non-parametric
    test should be used.
2.  **Homoscedasticity**\
    Another important assumption of the two-sample t-test is that the
    variance of your two samples are equal. This allows you to calculate
    a pooled variance, which in turn is used to calculate the standard
    error. If population variances are unequal, then the probability of
    a Type I error is greater than α.\
    The robustness of the t-test increases with sample size and is
    higher when groups have equal sizes.\
    We can test for difference in variances among two populations and
    ask what is the probability of taking two samples from two
    populations having identical variances and have the two sample
    variances be as different as are $s_{1}^2$ and $s_{2}^2$.\
    To do so, we must do the variance ratio test (i.e. an F-test).

#### Violation of assumptions

If variances between groups are not equal, it is possible to use
corrections, like the [Welch
correction](http://en.wikipedia.org/wiki/Welch's_t_test). If assumptions
cannot be respected, you can transform your data (log or square root for example) or use the non-parametric equivalent of t-test, the
[Mann-Whitney test](http://en.wikipedia.org/wiki/Mann–Whitney_U_test).
Finally, if the two groups are not independent (e.g. measurements on the
same individual at 2 different years), you should use a [Paired
t-test](http://en.wikipedia.org/wiki/Student's_t-test#Paired_samples).

#### Running a t-test

In R, t-tests are implemented using the function `t.test`. For example,
to test for a mass difference between aquatic and non-aquatic birds, you
should write:\

```{r, echo = TRUE, eval = FALSE}
# T-test
boxplot(logMass ~ Aquatic, data=bird, ylab=expression("log"[10]*"(Bird Mass)"),
        names=c("Non-Aquatic","Aquatic"),
        col=c("yellowgreen","skyblue"))

# First, let's test the assumption of equal variance
# Note: we do not need to test the assumption of normally distributed data since
# we already log transformed the data above
tapply(bird$logMass,bird$Aquatic,var)
var.test(logMass~Aquatic,data=bird)

# We are now ready to run the t-test
ttest1 <- t.test(Mass~Aquatic, var.equal=TRUE, data=bird)

# or equivalently
ttest1 <- t.test(x=bird$logMass[bird$Aquatic==0], y=bird$logMass[bird$Aquatic==1], var.equal=TRUE)
ttest1
```

![](images/lm_fig_10.png){width="400"}

      Two Sample t-test
     data:  logMass by Aquatic
     t = -7.7707, df = 52, p-value = 2.936e-10
     alternative hypothesis: true difference in means is not equal to 0
     95 percent confidence interval:
     -1.6669697 -0.9827343
     sample estimates:
     mean of x  mean of y
     1.583437   2.908289

Here, we show that the ratio of variances is not statistically different
from 1, therefore variances are equal, and we proceeded with our t-test.
Since p \< 0.05, the hypothesis of no difference between the two bird
types (Aquatic vs. terrestrial) was rejected.

#### Unilateral t-test

The *alternative* option of the `t.test` function allows for the use of
unilateral t-test. For example, if users want to test if non-aquatic
birds are less heavy than aquatic birds, the function can be written:

```{r, echo = TRUE, eval = FALSE}
# Alternative T-test
uni.ttest1 <- t.test(logMass~Aquatic, var.equal=TRUE, data=bird, alternative="less")
uni.ttest1
```

In the R output, called by `uni.ttest1`, the results of the t-test
appear in the third line:

      Two Sample t-test
      data:  logMass by Aquatic
      t = -7.7707, df = 52, p-value = 1.468e-10
      alternative hypothesis: true difference in means is less than 0
      95 percent confidence interval:
        -Inf -1.039331
      sample estimates:
      mean in group 0   mean in group 1
         1.583437          2.908289

In this case, the calculated t-statistic is t = -7.7707 with df = 52
degrees of freedom that gives a p-value of p-value = 1.468e-10. As the
calculated p-value is inferior to 0.05, the null hypothesis is rejected.
Thus, aquatic birds are significantly heavier than non-aquatic birds.

#### Running a t-test with lm()

A t-test is a linear model and a specific case of ANOVA with one factor
with 2 levels. As such, we can also run the t-test with the `lm()`
function in R:

```{r, echo = TRUE, eval = FALSE}
ttest.lm1 <- lm(logMass ~ Aquatic, data=bird)
anova(ttest.lm1)
```

      Analysis of Variance Table
      Response: logMass
                Df  Sum Sq  Mean Sq   F value    Pr(>F)
      Aquatic    1  19.015  19.0150   60.385     2.936e-10 ***
      Residuals 52  16.375  0.3149
      ---
      Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

When variances are equal (i.e., two-sample t-test), we can show that
t^2^ = F:

```{r, echo = TRUE, eval = FALSE}
ttest1$statistic^2
anova(ttest.lm1)$F
```

### 3.3 Running an ANOVA

The t-test is only for a single categorical explanatory variable with 2
levels. For all other linear models with categorical explanatory
variables we use ANOVA. First, let's visualize the data using
`boxplot()`. Recall that by default, R will order you groups in
alphabetical order. We can reorder the groups according to the median of
each Diet level.\
Another way to graphically view the effect sizes is to use
`plot.design()`. This function will illustrate the levels of a
particular factor along a vertical line, and the overall value of the
response is drawn as a horizontal line.

```{r, echo = TRUE, eval = FALSE}
# Default alphabetical order
boxplot(logMaxAbund ~ Diet, data=bird)

# Relevel factors
med <- sort(tapply(bird$logMaxAbund, bird$Diet, median))
boxplot(logMaxAbund ~ factor(Diet, levels=names(med)), data=bird, col=c("white","lightblue1",
           "skyblue1","skyblue3","skyblue4"))

plot.design(logMaxAbund ~ Diet, data=bird, ylab = expression("log"[10]*"(Maximum Abundance)"))
```

![](images/lm_fig_11.png){width="450"} ![](images/plot_design.png){width="350"}

Let's now run the ANOVA. In R, ANOVA can be called either directly with
the `aov` function, or with the `anova` function performed on a linear
model previously implemented with `lm`:

```{r, echo = TRUE, eval = FALSE}
# Using aov()
aov1 <- aov(logMaxAbund ~ Diet, data=bird)
summary(aov1)

# Using lm()
anov1 <- lm(logMaxAbund ~ Diet, data=bird)
anova(anov1)
```

### 3.4 Verifying assumptions

As with the simple linear regression and t-test, ANOVA must meet the
four assumptions of linear models. Below are some tips in how to test
these assumptions for an ANOVA.

1.  **Normal distribution**\
    The residuals of ANOVA model can once again be visualised in the
    normal QQplot. If the residuals lie linearly on the 1:1 line of the
    QQplot, they can be considered as normally distributed. If not, the
    ANOVA results cannot be interpreted.
2.  **Homoscedasticity**\
    To be valid, ANOVA must be performed on models with homogeneous
    variance of the residuals. This homoscedasticity can be verified
    using either the residuals vs fitted plot or the scale-location plot
    of the diagnostic plots. If these plots present equivalent spread of
    the residuals for each of the fitted values, then the residuals
    variance can be considered homogeneous.\
    A second way to assess the homogeneity of residuals variance is to
    perform a Bartlett test on the anova model using the function
    `bartlett.test`. If the p-value of this test is superior to 0.05,
    the null hypothesis H~0~: s~1~^2^ = s~2~^2^ =\... = s~j~^2^ =\... =
    s~n~^2^ is accepted and the homoscedasticity assumption is
    respected.\
    Usual transformations of explanatory variables can be used if the
    homogeneity of residuals variance is not met.
3.  **Additivity**\
    In addition to the assumption testing, it is important to consider
    whether the effects of two factors are additive. The effects are
    additive if the effect of one factor remains constant over all
    levels of the other factor, and that each factor influences the
    response variable independently of the other factor(s).

If assumptions are violated your can try to transform your data, which
could potentially equalize variances and normalize residuals, and can
convert a multiplicative effect into an additive effect. Or, if you
can't (or don't want to) transform your data, the non-parametric
equivalent of ANOVA is [Kruskal-Wallis
test](http://en.wikipedia.org/wiki/Kruskal–Wallis_one-way_analysis_of_variance).

```{r, echo = TRUE, eval = FALSE}
# Plot for diagnostics
opar <- par(mfrow=c(2,2))
plot(anov1)
par(opar)

# Test assumption of normality of residuals
shapiro.test(resid(anov1))

# Test assumption of homogeneity of variance
bartlett.test(logMaxAbund ~ Diet, data=bird)
```

Ideally the first diagnostic plot should show similar scatter for each
Diet level. The Shapiro and Bartlett tests are both non-significant,
therefore residuals are assumed to be normally distributed and variances
are assumed to be equal.

### 3.5 Model output

Once your ANOVA model has been validated, its results can be
interpreted. The R output of ANOVA model depends of the function that
has been used to implement the ANOVA. If the `aov` function is used to
implement the ANOVA model

```{r, echo = TRUE, eval = FALSE}
aov1 <- aov(logMaxAbund ~ Diet, data=bird)
```

the results of the ANOVA can be visualized using the function

```{r, echo = TRUE, eval = FALSE}
summary(aov1)
```

On the other hand, if `lm()` is used

```{r, echo = TRUE, eval = FALSE}
anov1 <- lm(logMaxAbund ~ Diet, data=bird)
```

the ANOVA results must be called using the function

```{r, echo = TRUE, eval = FALSE}
anova(anov1)
```

In both cases, the R output is as follows:

              Df  Sum Sq      Mean Sq     F value     Pr(>F)
    Diet          4   5.106       1.276       2.836       0.0341 *
    Residuals     49  22.052      0.450
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

This R output corresponds exactly to the ANOVA table of your model. This
output also present the degrees of freedom, the sum of squares, the mean
sum of squares and the F-value previously explained. For this example,
the diet significantly influences the abundance of birds as the p-value
is inferior to 0.05. The null hypothesis can then be rejected meaning
that at least one of the diet treatments influenced the abundance
differently than the other treatments.

### 3.6 Complementary test

Importantly, ANOVA cannot identify which treatment is different from the
others in terms of response variable. It can only identify that a
difference is present. To determine the location of the difference(s),
post-hoc tests that compare the levels of the explanatory variables
(i.e. the treatments) two by two, must be performed. While several
post-hoc tests exist (e.g. Fischer's least significant difference,
Duncan's new multiple range test, Newman-Keuls method, Dunnett's test,
etc.), the Tukey's range test is used in this example using the function
`TukeyHSD` as follows:

```{r, echo = TRUE, eval = FALSE}
# Where does the Diet difference lie?
TukeyHSD(aov(anov1),ordered=T)

# or equivalently
TukeyHSD(aov1,ordered=T)
```

The R output for this test gives a table containing all the two by two
comparisons of the explanatory variable levels and identify which
treatment differ from the others:\

    Tukey multiple comparisons of means 95% family-wise confidence level
       factor levels have been ordered
    Fit: aov(formula = anov1)
    $Diet
                                  diff             lwr        upr         p adj
    Vertebrate-InsectVert     0.3364295   -1.11457613     1.787435    0.9645742
    Insect-InsectVert         0.6434334   -0.76550517     2.052372    0.6965047
    Plant-InsectVert          0.8844338   -1.01537856     2.784246    0.6812494
    PlantInsect-InsectVert    1.0657336   -0.35030287     2.481770    0.2235587
    Insect-Vertebrate         0.3070039   -0.38670951     1.000717    0.7204249
    Plant-Vertebrate          0.5480043   -0.90300137     1.999010    0.8211024
    PlantInsect-Vertebrate    0.7293041   0.02128588  1.437322    0.0405485
    Plant-Insect              0.2410004   -1.16793813     1.649939    0.9884504
    PlantInsect-Insect        0.4223003   -0.19493574     1.039536    0.3117612
    PlantInsect-Plant         0.1812999   -1.23473664     1.597336    0.9961844

In this case, the only significant difference in abundance occurs
between the PlantInsect diet and the Vertebrate diet.

### 3.7 Plotting

After having verified the assumptions of your ANOVA model, interpreted
the ANOVA table and differentiated the effect of the treatments using
post-hoc tests or contrasts, the ANOVA results can be graphically
illustrated using a `barplot`. This shows the response variable as a
function of the explanatory variable levels, where standard errors can
be superimposed on each bar as well as the different letters
representing the treatment group (according to the post-hoc test).

```{r, echo = TRUE, eval = FALSE}
# Graphical illustration of ANOVA model using barplot()

sd <- tapply(bird$logMaxAbund,list(bird$Diet),sd)
means <- tapply(bird$logMaxAbund,list(bird$Diet),mean)
n <- length(bird$logMaxAbund)
se <- 1.96*sd/sqrt(n)

bp <- barplot(means, col=c("white","lightblue1","skyblue1","skyblue3","skyblue4"),
       ylab = expression("log"[10]*"(Maximum Abundance)"), xlab="Diet", ylim=c(0,1.8))

# Add vertical se bars
segments(bp, means - se, bp, means + se, lwd=2)
# and horizontal lines
segments(bp - 0.1, means - se, bp + 0.1, means - se, lwd=2)
segments(bp - 0.1, means + se, bp + 0.1, means + se, lwd=2)
```

![](images/lm_fig_12.png){width="650"}

### Going further: Contrasts

-   Contrasts are group mean comparisons based on an a priori
    hypothesis,
-   These groups can be compounded of one or many levels of a factor ,
-   We can test basic hypothesis (ex: μ~1~ = μ~2~) or more complex
    hypothesis (ex: (μ~1~ + μ~2~)/3 == μ~3~).

The number of comparisons has to be lower or equal to the number of
degrees of freedom of the ANOVA. Comparisons have to be independent from
one another.

Contrasts are given as an R output for ANOVA can be called to visualize
the parameter estimates for each level of the explanatory variable in
comparison to a baseline level. This output is called using the function
`summary.lm(aov1)` when the ANOVA model was implemented with the `aov`
function, and using the `summary(anov1)` when the ANOVA was implemented
with the `lm` function. This output performs a linear regression for
each level of the explanatory variable and calculates their associated
parameters:

    Call:     lm(formula = logMaxAbund ~ Diet, data = bird)
    Residuals:
          Min         1Q          Median          3Q          Max
         -1.85286     -0.32972    -0.08808    0.47375     1.56075
    Coefficients:
                      Estimate    Std. Error  t value     Pr(>|t|)
    (Intercept)       1.1539      0.1500      7.692   5.66e-10 ***
    DietInsectVert   -0.6434      0.4975     -1.293   0.2020
    DietPlant         0.2410      0.4975      0.484   0.6303
    DietPlantInsect   0.4223      0.2180      1.938       0.0585 .
    DietVertebrate   -0.3070      0.2450     -1.253       0.2161
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
    Residual standard error: 0.6709 on 49 degrees of freedom
    Multiple R-squared:  0.188,     Adjusted R-squared:  0.1217
    F-statistic: 2.836 on 4 and 49 DF,  p-value: 0.0341

Users can note that the last line of this R output corresponds exactly
to the previous R output shown in the ANOVA table. The F-statistic of
the ANOVA model and its associated p-value (i.e. 2.836 and 0.0341), are
the same one as the values obtained from the ANOVA table, indicating
diet explains the abundance better than a null model, and so, diet
significantly influence the abundance. The goodness-of-fit of the ANOVA
model (i.e. adjusted R-square value) appears in the second to last line
of this output. In this case, diet explains 12.17% of the abundance
variability.\
Contrasts adjust a linear regression of the response variable as
function of each level of the categorical explanatory variable
separately. In this case, 5 linear regressions (corresponding to the
five lines of the coefficients table of the R output) are calculated by
the lm function as the diet variable contains 5 levels. By default, the
baseline level corresponding to the intercept is the first level of
explanatory variables ranked in alphabetical order. So in this case, the
"Insect" diet is automatically used as a baseline in R. In the R output,
the coefficient estimate of the baseline level (here, 1.1539) is first
compared to 0 using a t-test (in this case, the t-test is significant
with a p-value of 5.66e-10), while the coefficient estimates of the
other explanatory variable level are compared to the baseline level. In
this case, only the PlantInsect diet differs from the Insect diet, with
an associated p-value of 0.0585.\
In other words, this R output allows us to determine the mean of the
response variable for each of the diet levels, for example:

LogMaxAbund = 1.1539 for the Insect diet,\
LogMaxAbund = 1.1539 -- 0.6434 for the InsectVert diet,\
LogMaxAbund = 1.1539 + 0.2410 for the Plant diet,\
etc.

As this type of contrasts compares each level of the explanatory
variable to a baseline level, they are called contr.treatment and
constitute the default method of the `lm` function in R. The baseline
level can, however, be changed using the `relevel` function. For
example,

```{r, echo = TRUE, eval = FALSE}
bird$Diet2 <- relevel(bird$Diet, ref="Plant")
anov_rl <- lm(logMaxAbund ~ Diet2, data=bird)
summary(anov_rl)
anova(anov_rl)
```

compares each diet treatment to the Plant diet, now defined as the
baseline level.

The contrasts coefficient matrix of these contr.treatment comparisons
can be called by

```{r, echo = TRUE, eval = FALSE}
contrasts(bird$Diet2)
```

                  Insect      InsectVert  PlantInsect     Vertebrate
    Plant             0           0           0                   0
    Insect            1           0           0                   0
    InsectVert              0             1           0                   0
    PlantInsect             0             0           1                   0
    Vertebrate            0           0           0               1

where each column corresponds to a comparison to the baseline Plant and
each line to a diet level. For example, the first comparison compares
the Insect diet to the Plant diet, the second one compares the
InsectVert diet to the Plant diet, etc.

Users can create their own contrasts coefficient matrix in order to
perform the comparison they desire using the contrasts function. For
example,

```{r, echo = TRUE, eval = FALSE}
contrasts(bird$Diet2) <- cbind(c(4,-1,-1,-1,-1), c(0,1,1,-1,-1), c(0,0,0,1,-1), c(0,1,-1,0,0))
```

creates this contrasts coefficient matrix:

                  [,1]    [,2]    [,3]    [,4]
    Plant          4       0       0       0
    Insect        -1       1       0       1
    InsectVert    -1       1       0      -1
    PlantInsect   -1      -1       1       0
    Vertebrate    -1      -1      -1       0

that compares:\

-   the Plant diet to all the other diets in the first comparison,\
    \* the InsectVert and the Insect diets to the PlantInsect and the
    Vertebrate diets in the second one,\
    \* the PlantInsect diet to the Vertebrate diet in the third one,\
    \* and the Insect diet to the InsectVert diet in the fourth one.

Thus, for each column, the treatments with identical contrasts
coefficient belong to the same two by two comparison group (e.g. in the
column 1, the four treatment with a -1 coefficient belong to the first
comparison group and are altogether compare to the second group
corresponding to the treatment with a different coefficient, here the
Plant diet with a coefficient 4). Thus, all comparison possible can be
performed with personalized contrasts coefficient matrix. Only two
conditions restrict the possible contrasts coefficient matrix:

      - for each column, the sum of the coefficients must be equal to 0, and
      - the sum of the product of each pair of column must be equal to 0.

This can be verified using the following R commands

```{r, echo = TRUE, eval = FALSE}
sum(contrasts(bird$Diet)[,1]) # first condition for column 1
sum(contrasts(bird$Diet)[,1]*contrasts(bird$Diet)[,2]) # second condition for column 1 and 2
```

Conventional contrast coefficient matrices are already programmed in R,
such as the contr.helmert or the contr.poly matric (cf.
`help(contrasts)` ).

## 4. Two-way ANOVA

In the above section, the ANOVA models had a single categorical
variable. We can create ANOVA models with multiple categorical
explanatory variables. When there are two categorical explanatory
variables, we refer to the model as a two-way ANOVA. A two-way ANOVA
tests several hypotheses: that there is no difference in mean among
levels of variable A; that there is no difference in mean among levels
of variable B; and that there is no interaction between variables A and
B. A significant interaction means the mean value of the response
variable for each level of variable A changes depending on the level of
B. For example, perhaps relationship between the colour of a fruit and
its mass will depend on the plant species: if so, we say there is an
interaction between colour and species.

Click to see the math in more detail below.  The one-way ANOVA
table has to be rewritten to add the second explanatory term as well as
the interaction term. Thus, a two-way ANOVA table corresponds to:

  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  Source of\      Degrees of\             Sums of squares                                                                                Mean squares   F-statistic
  variation       freedom (df)
  --------------- ----------------------- ---------------------------------------------------------------------------------------------- -------------- -------------------------------------------- --------------------------------------------
  Total           $abr-1$       ${SS_Tot}=sum{i,j,k}{}({y_ijk}-overline{y})                                                2$

  Cells           $ab-1$        ${SS_Cells}= sum{i,j}{}({overline{y}\_ij}-overline{y})                                     2$

  Within-\        $ab(r-1)$     ${SS_E}= sum{i,j,k}{}({y}\_ijk-{overline{y}\_ij})                                          2$       ${MS_E}={SS_E}/{ab(r-1)}$
  cells (error)

  Factor A        $a-1$         ${SS_FactorA}= rb sum{i}{}({overline{y}\_i.}-overline{y})                                  2$       ${MS_FactorA}={SS_FactorA}/{a-1}$  ${F_FactorA}={MS_FactorA}/{MS_E}\</m\>

  Factor B        $b-1$         ${SS_FactorB}= ra sum{j}{}({overline{y}\_.j}-overline{y})                                  2$       ${MS_FactorB}={SS_FactorB}/{b-1}$  ${F_FactorB}={MS_FactorB}/{MS_E}\</m\>

  Interaction\    $(a-1)(b-1)$  ${SS_AB}= r sum{i,j,k}{}({overline{y}\_{..k}}-{overline{y}\_{.jk}}-{overline{y}\_{i.k}})   2$       ${MS_AB}={SS_AB}/{(a-1)(b-1)}$     ${F_AB}={MS_AB}/{MS_E}\</m\>
  AB
  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

a: number of levels of the explanatory variable A; b: number of levels
of the explanatory variable B; r: number of replicates per treatment


### 4.1 Running a two-way ANOVA

In R, a two-way ANOVA model is implemented in the same fashion as a
one-way ANOVA using the function `lm`.

------------------------------------------------------------------------

**CHALLENGE 2**

Examine the effects of the factors Diet, Aquatic, and their interaction
on the maximum bird abundance.

Recall: Before interpreting the ANOVA results, the model must first be
validated by verifying the statistical assumptions of ANOVA, namely
the:\
- Normal distribution of the model residuals

1.  Homoscedasticty of the residuals variance

This verification can be done using the four diagnostic plots as
previously explained for one-way ANOVA.\
++++ Challenge 2: Solution \|

```{r, echo = TRUE, eval = FALSE}
anov2 <- lm(logMaxAbund ~ Diet*Aquatic, data=bird)
opar <- par(mfrow=c(2,2))
plot(anova2)
par(opar)
summary(anov2)
anova(anov2)
```

The `anova` command enables one to visualize the ANOVA table of the
model:\

    Analysis of Variance Table
    Response: logMaxAbund
                  Df  Sum Sq Mean Sq  F value Pr(>F)
    Diet              4  5.1059  1.27647  3.0378  0.02669 *
    Aquatic           1  0.3183  0.31834  0.7576  0.38870
    Diet:Aquatic      3  2.8250  0.94167  2.2410  0.09644 .
    Residuals     45 18.9087 0.42019
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

In this case, the only significant term of the model is the diet factor
as the p-value associated with the interaction term is not significant,
meaning that the synergic effect of aquatic/non-aquatic type of birds
does not influence the abundance. The significance of the interaction
can also be determined by comparing two nested ANOVA models, i.e. a
first model with the interaction and a second model without the
interaction, using the `anova` command:

```{r, echo = TRUE, eval = FALSE}
anov3 <- lm(logMaxAbund ~ Diet + Aquatic, data=bird)
anova(anov3, anov2)
```

which gives the following output:

     Analysis of Variance Table
     Model 1: logMaxAbund ~ Diet + Aquatic
     Model 2: logMaxAbund ~ Diet * Aquatic
       Res.Df     RSS     Df  Sum of Sq       F   Pr(>F)
     1     48         21.734
     2     45 1       8.909   3       2.825       2.241   0.09644 .
     ---
     Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

As the only difference between the two compared ANOVA models is the
presence of the interaction term, this R output presents the
significance of this term. In this case, the interaction term is not
significant and can be dropped from the final ANOVA model.\
When the interaction term is significant, users should remember that the
single effect of each explanatory variable cannot be interpreted and
only the interaction term can.

[**Note:**]{.ul}The ANOVA table reports that the number of DF (degrees
of freedom) for the interaction between Diet:Aquatic is 3. According to
the notation in the two-way ANOVA table (for balanced designed), a = 5
and b = 2 and the DF for the interaction should be (a-1)(b-1) = 4\*1 =
4. The R output provides a DF of 3 because the Diet\*Aquatic treatment
is extremely unbalanced. Namely, since there is no aquatic birds that
feed on plants we loose that interaction (note the NA in the
summary(anov4) output). Please refer to the advanced section on
[Unbalanced
ANOVA](http://qcbs.ca/wiki/r_workshop4?&#unbalanced_anova_advanced_section_optional)
below for further details. ++++

------------------------------------------------------------------------

### 4.2 Interaction plot

Interactions can also be viewed graphically using the function
`interaction.plot` as:

```{r, echo = TRUE, eval = FALSE}
interaction.plot(bird$Diet, bird$Aquatic, bird$logMaxAbund, col="black",
                 ylab = expression("log"[10]*"(Maximum Abundance)"), xlab="Diet")
```

![](images/interaction_plot.png){width="650"}

What do the gaps in the line for the Aquatic group mean?

```{r, echo = TRUE, eval = FALSE}
table(bird$Diet, bird$Aquatic)
```

                 0  1
    Insect      14  6
    InsectVert   1  1
    Plant        2  0
    PlantInsect 17  1
    Vertebrate   5  7

The design is unbalanced; unequal observations among diet levels for
Aquatic (coded as 1) and Terrestrial (coded as 0). See advanced section
below for details on unbalanced ANOVA designs.

------------------------------------------------------------------------

**CHALLENGE 3**

Test the significance of the Aquatic factor by comparing nested models
with and without this categorical variable.

++++ Challenge 3: Solution \|

```{r, echo = TRUE, eval = FALSE}
anova(anov1,anov3) # Recall: anov1 is model with only Diet as a factor
```

++++

------------------------------------------------------------------------

## 5. Unbalanced ANOVA (advanced section/ optional)

 One-way and two-way ANOVA enabled us to determine the effect
of categorical explanatory variables on a continuous response variable
for the case of balanced experimental designs (i.e. when all levels of
the explanatory variables contain the same number of replicates).
However, loss of experimental units over the course of an experiment, or
technical restriction of experimental designs can result in unbalanced
designs. In this case, the above-mentioned ANOVA tests lead to
misleading results related due to incorrect sum of squares calculations.
For unbalanced experimental design, ANOVA must be modified to correctly
account for the missing values of the response variable.

While mathematical model, statistical hypothesis and statistical
assumptions for ANOVA with unbalanced designs remain the same as for
ANOVA with balanced designs the sum of squares calculation changes.

For unbalanced design, ANOVA thus test the hypothesis:

H~0~: µ~1~ = µ~2~ =\... = µ~i~ =\... = µ~n~\
H~1~: there is at least one µi that differs from the others.

Using this mathematical model:

${y\_{ijk}} = µ + {A_i} + {B_j} + {A_i}{B_j} + {ε\_{ijk}}\</m\>

Recall the sum of squares calculation of the ANOVA with balanced design:

$ {SS_FactorA} = rb sum{i}{}{({overline{y}\_{i.}}-overline{y})}\^2 =
SS(A) \</m\>\
$ {SS_FactorB} = ra sum{j}{}{({overline{y}\_{.j}}-overline{y})}\^2 =
SS(B\|A) = SS(A,B)-SS(B) \</m\>\
$ {SS_AB} = r
sum{i,j,k}{}({overline{y}\_{..k}}-{overline{y}\_{.jk}}-{overline{y}\_{i.k}})\^2
= SS(A,B,AB)-SS(A,B) \</m\>

This corresponds to sequential sum of squares, or type I sum of squares,
as the main effect of B is calculated after removing the main effect of
A, and the interaction effect is calculated after removing the two main
effects. These calculations are sample size dependent as the effect of
each factor is calculated after removing the effect of the precedent
factor.

For unbalanced design, ANOVA results will depend on the order in which
each explanatory variable appears in the model. This can be seen by
comparing the results of the following two models:

```{r, echo = TRUE, eval = FALSE}
unb_anov1 <- lm(logMaxAbund ~ Aquatic + Diet, data=bird)
unb_anov2 <- lm(logMaxAbund ~ Diet + Aquatic, data=bird)
anova(unb_anov1)
anova(unb_anov2)
```

While the same explanatory variables are used in these two models, the
ANOVA tables show different results due to the unbalanced design (i.e.
different number of observations for aquatic and non-aquatic birds).

For unbalanced designs, marginal sum of squares, or type III sum of
squares that perform calculations of main effect after removing the
effect of all other factors ensure independence from sample size
effects:

${SS_FactorA}={SS(A\|B,AB)}=SS(A,B,AB)-SS(B,AB)\</m\>\
${SS_FactorB}={SS(B\|A,AB)}=SS(A,B,AB)-SS(A,AB)\</m\>\
${SS_AB}={SS(AB\|B,A)}=SS(A,B,AB)-SS(B,AB) \</m\>\

In R, ANOVA with type III sum of squares can be implemented using the
Anova function of package 'car' and specifying "III" in the type option,
for example:

```{r, echo = TRUE, eval = FALSE}
Anova(unb_anov1,type="III")
```

By comparing ANOVA tables of models with different order of explanatory
variables, users can now note that the ANOVA table results always remain
the same. Type III sum of squares thus correctly calculates the ANOVA
results by becoming independent of sample sizes.

After having verifying the model assumptions, results can finally be
safely interpreted.