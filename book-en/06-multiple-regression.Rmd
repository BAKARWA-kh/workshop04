# Multiple linear regression

![](images/schema_multReg.png)

A multiple regression tests the effects of several continuous explanatory variables on a continuous response variable. It differs from simple linear regression by having *more than one explanatory variable*.

### Model formulation

**Variables**

The multiple linear regression is defined by the variables $y$ representing the response variable (**continuous**) and $x$ for the explanatory variables (**continuous** or **categorical**).

**The assumed relationship**

The relationship between the response variable and the predictors is defined in the same way as for simple regression. The difference is in the addition of $\beta$ parameters for the additional variables:

$$y_i = \beta_0 + \beta_1x_{1,i}+ \beta_2x_{2,i}+ \beta_3x_{3,i}+...+ \beta_kx_{k,i} + \epsilon_i$$

* The parameter $\beta_0$ is **the intercept** (or constant)
* The parameter $\beta_1$ quantifies **the effect** of $x$ on $y$
* The residual $\epsilon_i$ represents the **unexplained** variation
* The **predicted value** of $y_i$ is defined as: $\hat{y}_i = \beta_0 + \beta_1x_{1,i}+ \beta_2x_{2,i}+ \beta_3x_{3,i}+...+\beta_kx_{k,i}$.

The unexplained variation or error remains normally distributed, centered on zero with a variance of $\sigma^2$ :

$$epsilon_i \sim \mathcal{N}(0,\,\sigma^2)$$

## Assumptions

------------------------------------------------------------------------

In the case of multiple linear regressions, two conditions are added to the usual conditions for linear models. First, there must be a **linear relationship** between **each** explanatory variable and the response variable. Second, the explanatory variables are independent of each other (there is no **colinearity**).

### If variables are collinear

In case of collinearity, there are some solutions:

* Keep only one of the variables collinear
* Try a multidimensional analysis (see workshop 9)
* Try a pseudo-orthogonal analysis

##  Multiple linear regression in R

------------------------------------------------------------------------

### The data

Using the `Dickcissel` dataset we will compare the relative importance of climate (`clTma`), productivity (`NDVI`) and soil cover (`grass`) as predictors of dickcissel abundance (`abund`).

```{r, eval=TRUE}
Dickcissel = read.csv("data/dickcissel.csv")
str(Dickcissel)
```

### Verify assumptions

First, we must verify the presence of **colinearity** between all the explanatory and interest variables:

:::explanation
An observable pattern between two explanatory variables may indicate that they are **colinear**! You must avoid this, or their effects on the response variable will be confounded.
:::

### Linear regression

Now, let's run the multiple regression of abundance (`abund`) against the variables `clTma + NDVI + grass` :

```{r}
# Multiple regression
lm.mult <- lm(abund ~ clTma + NDVI + grass, data = Dickcissel)
summary(lm.mult)
```

Then, let us check the other assumptions, as for the simple linear regression:

```{r, fig.height=5.5, fig.width=8,echo=-2}
# Assumptions
par(mfrow = c(2, 2))
par(mfrow=c(2,2), mar = c(3.9,4,1.2,1.1), oma =c(0,0,0,0))
plot(lm.mult)
```

### Find the best-fit model

There is a principle of primary importance in model selection. It is the **principle of parsimony**. That is, explain the most variation with the least number of terms. We could therefore remove the least significant variable.

```{r}
summary(lm.mult)$coefficients
```

All 3 variables are important. We keep everything!

The model explains 11.28% of the variability in dickcissel abundance $RÂ²_{adj} = 0.11$.

:::noway
However, this information is not valid, because the conditions for applying the linear model are not met.
:::

It is important to note that the response variable does not vary linearly with the explanatory variables:

```{r, fig.height=3.5, fig.width=11,echo=-1}
# Plot the response vs predictor variables
par(mfrow=c(1,3), mar=c(4, 4, 0.5, 0.5), cex = 1)
plot(abund ~ clTma, data = Dickcissel)
plot(abund ~ NDVI,  data = Dickcissel)
plot(abund ~ grass, data = Dickcissel)
```