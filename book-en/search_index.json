[["index.html", "Workshop 4: Linear models QCBS R Workshop Series Preface 0.1 Code of conduct 0.2 Contributors 0.3 Contributing", " Workshop 4: Linear models QCBS R Workshop Series Developed and maintained by the contributors of the QCBS R Workshop Series1 2022-03-23 22:26:16 Preface The QCBS R Workshop Series is a series of 10 workshops that walks participants through the steps required to use R for a wide array of statistical analyses relevant to research in biology and ecology. These open-access workshops were created by members of the QCBS both for members of the QCBS and the larger community. The content of this workshop has been peer-reviewed by several QCBS members. If you would like to suggest modifications, please contact the current series coordinators, listed on the main Github page. 0.1 Code of conduct The QCBS R Workshop Series and the QCBS R Symposium are venues dedicated to providing a welcoming and supportive environment for all people, regardless of background or identity. Participants, presenters and organizers of the workshop series and other related activities accept this Code of Conduct when being present at any workshop-related activities. We do not tolerate behaviour that is disrespectful or that excludes, intimidates, or causes discomfort to others. We do not tolerate discrimination or harassment based on characteristics that include, but are not limited to, gender identity and expression, sexual orientation, disability, physical appearance, body size, citizenship, nationality, ethnic or social origin, pregnancy, familial status, genetic information, religion or belief (or lack thereof), membership of a national minority, property, age, education, socio-economic status, technical choices, and experience level. It applies to all spaces managed by or affiliated with the workshop, including, but not limited to, workshops, email lists, and online forums such as GitHub, Slack and Twitter. 0.1.1 Expected behaviour All participants are expected to show respect and courtesy to others. All interactions should be professional regardless of platform: either online or in-person. In order to foster a positive and professional learning environment we encourage the following kinds of behaviours in all workshop events and platforms: Use welcoming and inclusive language Be respectful of different viewpoints and experiences Gracefully accept constructive criticism Focus on what is best for the community Show courtesy and respect towards other community members 0.1.2 Unacceptable behaviour Examples of unacceptable behaviour by participants at any workshop event/platform include: written or verbal comments which have the effect of excluding people on the - basis of membership of any specific group; causing someone to fear for their safety, such as through stalking or intimidation; violent threats or language directed against another person; the display of sexual or violent images; unwelcome sexual attention; nonconsensual or unwelcome physical contact; insults or put-downs; sexist, racist, homophobic, transphobic, ableist, or exclusionary jokes; incitement to violence, suicide, or self-harm; continuing to initiate interaction (including photography or recording) with - someone after being asked to stop; publication of private communication without consent. 0.2 Contributors This workshop was originally developed by Catherine Baltazar, Bérenger Bourgeois, Zofia Taranu, and Shaun Turney. Since 2014, several QCBS members contributed to consistently and collaboratively develop and update this workshop, as part of the Learning and Development Award from the Québec Centre for Biodiversity Science. They were: 2022 - 2021 - 2020 2019 - 2018 - 2017 2016 - 2015 - 2014 Victor Cameron Willian Vieira Catherine Baltazar Laurie Maynard Shaun Turney Bérenger Bourgeois Daniel Schoenig Marie-Hélène Brice Zofia Taranu Katherine Hébert Shaun Turney Emmanuelle Chrétien Maxwell Farrell Vincent Fugère 0.3 Contributing Under construction. The QCBS R Workshop Series is part of the Québec Centre for Biodiversity Science, and is maintained by the series coordinators and graduent student, postdoctoral, and research professional members. The contributors for this workshop can be accessed here.↩︎ "],["learning-objectives.html", "Chapter 1 Learning objectives", " Chapter 1 Learning objectives Summary : In this workshop, you will learn the structure of a linear model and its different variants such as simple regression, analysis of variance (ANOVA), analysis of covariance (ANCOVA) and multiple regression. This workshop focuses on implementing linear models in R with lm() and anova(), identifying models that do not meet the conditions for application and solving problems. "],["preparing-for-the-workshop.html", "Chapter 2 Preparing for the workshop", " Chapter 2 Preparing for the workshop All workshop materials are found at github.com/QCBSRworkshops/workshop04. This includes an R script which contains all code chunks shown in this book. For this workshop, we will be working with the following datasets: birdsdiet dickcissel To download this data, do right click + save on the page that opens. You should also make sure you have downloaded, installed, and loaded these packages: dplyr (to work with data) vegan (for population data analyses) e1071 (to test models) MASS (for linear discriminant analysis) car (for the analysis of variance) effect (to visualize models) To install them from CRAN, run: # Install the required packages install.packages(&quot;dplyr&quot;) install.packages(&quot;vegan&quot;) install.packages(&quot;e1071&quot;) install.packages(&quot;MASS&quot;) install.packages(&quot;car&quot;) install.packages(&quot;effects&quot;) To load these packages, run: # Load the required packages library(dplyr) library(vegan) library(e1071) library(MASS) library(car) library(effects) "],["the-linear-model.html", "Chapter 3 The linear model 3.1 What is a linear model? 3.2 Formulation of a linear model 3.3 Evaluation of the linear model 3.4 Assumptions of the linear model 3.5 Notation for linear models 3.6 Fitting a linear model 3.7 Learning objectives", " Chapter 3 The linear model The previous workshops explored the use of R to manipulate and represent data. However, the focus in science is often on determining relationships between variables. This workshop builds on previous knowledge and focuses on linear regression as a first step into the world of statistical analysis. Learning Objectives Learn the structure of a linear model and its different variants. Learn how to make a linear model in R with lm() and anova(). Learn how to identify a model whose conditions are not satisfied and how to solve the problem. 3.1 What is a linear model? A linear model describes the relationship between a response variable and one or more predictor variables. It is used to analyze a well-formed hypothesis, often associated with a more general research question. Regression determines whether variables are correlated by inferring the direction and strength of a relationship, and our confidence in the effect size estimates. Significant scientific work is required to formulate a linear model. Since the model is analyzing a hypothesis, it is recommended that expectations about the direction and strength of a relationship be clearly articulated as predictions before running a linear model. 3.1.1 Example: Abundance and mass of bird species Hypothesis For different bird species, the average mass of an individual affects the maximum abundance of the species, due to ecological constraints (food sources, habitat availability, etc.). Prediction Species characterized by larger individuals have lower maximum abundance. We therefore hypothesize that birds characterized by greater mass require more food and space: the response variable is maximum abundance, and the predictor is the average weight of an individual. Based on our prediction, we can expect the direction of the relationship between the response variable and the predictor to be inverse or ‘negative’, so that higher mass leads to lower abundance. However, we cannot formulate expectations about the strength of the relationship! The data The following example explores the “birdsdiet” dataset: # Import the &#39;bidsdiet&#39; dataset and save it in the &#39;bird&#39; # object bird &lt;- read.csv(&quot;data/birdsdiet.csv&quot;, stringsAsFactors = TRUE) The bird dataset contains seven variables: # Explore the variables in the &#39;bird&#39; dataset str(bird) ## &#39;data.frame&#39;: 54 obs. of 7 variables: ## $ Family : Factor w/ 53 levels &quot;Anhingas&quot;,&quot;Auks&amp; Puffins&quot;,..: 18 25 23 21 2 10 1 44 24 19 ... ## $ MaxAbund : num 2.99 37.8 241.4 4.4 4.53 ... ## $ AvgAbund : num 0.674 4.04 23.105 0.595 2.963 ... ## $ Mass : num 716 5.3 35.8 119.4 315.5 ... ## $ Diet : Factor w/ 5 levels &quot;Insect&quot;,&quot;InsectVert&quot;,..: 5 1 4 5 2 4 5 1 1 5 ... ## $ Passerine: int 0 1 1 0 0 0 0 0 0 0 ... ## $ Aquatic : int 0 0 0 0 1 1 1 0 1 1 ... Note that Family, Diet, Passerine, and Aquatic are all categorical variables, despite the fact that they are encoded in different ways (string, categorical, binary). The variables of interest to the hypothesis test are: “MaxAbund”: The greatest abundance observed at a site in North America (continuous / numeric) “Mass”: The average body size in grams (continuous / numeric) **Basic Concepts There are two key concepts for understanding linear models: localization and variation. Beware! These concepts are central to understanding the other concepts presented in the workshop. The localization is a measure of central tendency of a population. It can be measured with the arithmetic mean \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}\\) : # Average maximum observed abundance mean(bird$MaxAbund) ## [1] 44.90577 or the median: # Median of maximum observed abundance median(bird$MaxAbund) ## [1] 24.14682 On the other hand, the mean cannot characterize an entire population. We can therefore also describe a population using measures of variation. Variation is the dispersion (or deviation) of observations around the mean. It is measured with the variance \\(\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} {(x_{i} - \\bar{x})}^2\\). The variance is the sum of the squared deviation between each value and the mean. Squaring the variance allows us to transform values into positive values without using absolute values. # Variance of the maximum observed abundance var(bird$MaxAbund) ## [1] 5397.675 On the other hand, squaring all values changes the units of our variables. In this example, the variance is given in abundance^2, a unit that is no longer relevant to our original question. To transform these values into the appropriate units, we can calculate the standard deviation \\(\\sigma\\). # Standard deviation of the maximum observed abundance sd(bird$MaxAbund) ## [1] 73.46887 Visual exploration The following figure shows the response variable against the predictor: # plot the response in relation to the predictor plot(bird$Mass, bird$MaxAbund) This figure shows the variation of maximum abundance with bird mass. We are interested in quantifying this relationship. In other words, we are interested in quantifying the effect of bird species mass on their maximum abundance. However, how do we find the “best” estimate of the relationship? The “best” estimate is the line that minimizes the sum of squares. This will become clearer in the next sections. Note that the “best” estimate can also be the absence of a relationship, similar to the blue line in the figure. 3.2 Formulation of a linear model The linear model describes the “best” relationship between a response variable (“MaxAbund”) and one or more predictors (here “Mass”). The response variable is the variable we want to explain, or the dependent variable. There is only one response variable. The explanatory variables are variables that can (potentially) explain the response variable. One or more explanatory variables can be included. In a linear model, we define an observation of the response variable \\(y\\), as \\(y_i\\). In our example, this represents an observation of maximum abundance for a species \\(i\\). A corresponding observation of the predictor \\(x\\) is defined as \\(x_i\\) and represents, for example, the average weight of an individual of a species \\(i\\). In linear models, the concepts of mean and variation are used to describe the relationship between two variables. We say “linear models”, because they describe the relationship between variables with a straight line. This line represents the assumed relationship: \\[ y_i = \\beta_0 + \\beta_1 \\times x_i + \\epsilon_i\\] \\(y_i\\) is the response variable \\(x_i\\) is the predictor The parameter \\(\\beta_0\\) is the intercept. The parameter \\(\\beta_1\\) quantifies the effect** of \\(x\\) on \\(y\\) The residual \\(\\epsilon_i\\) represents the unexplained variation The predicted value of \\(y_i\\) is defined as: \\(hat{y}_i = \\beta_0 + \\beta_1 \\times x_i\\) 3.3 Evaluation of the linear model The linear model evaluates whether there is a significant correlation between the response variable and the explanatory variable(s). This is done by assessing whether the mean value of the response variable differs significantly between different values of the explanatory variables, such as for variation in maximum bird abundance by mass. For categorical explanatory variables, the linear model assesses whether the mean value of the response variable differs significantly between different levels (or groups) of explanatory variables. This will become clearer as we explore the types of linear models in the following sections. In almost all cases, the explanatory variables do not explain all the variation in the response variable. The variation that remains unexplained is the residuals, or error. 3.4 Assumptions of the linear model The linear model is defined according to the equation we explored earlier: \\[ y_i = \\beta_0 + \\beta_1 \\times x_i + \\epsilon_i\\] To be valid, all linear models rely on 4 basic assumptions. If the 4 assumptions are not met, the model results cannot be interpreted in a valid way. Linear relationship between the response and the predictor Residuals follow a normal distribution with a mean of $0 Residuals are symmetrically distributed (homoscedasticity) Residuals are independent of each other Note that these 4 conditions concern the residuals, not the response or explanatory variables. This does not mean that all observed \\(y\\) values must follow a normal distribution! 3.4.1 Normal distribution of the residuals For the results of a linear model to be valid, the residuals must follow a normal distribution with a mean of \\(0\\) and a variance of \\(\\sigma^2\\), so that the majority of the residuals have a value close to 0 (i.e. the error is very small) and their distribution is symmetrical (i.e. the response variable is underestimated as well as overestimated): \\[\\epsilon_i \\sim \\mathcal{N}(0,\\,\\sigma^2)\\] This means: Each obsevation \\(y_i\\) follows a normal distribution, with mean \\(\\hat{y} = \\beta_0 + \\beta_1 \\times x_i\\) and variance \\(\\sigma^2\\): \\[y_i \\sim \\mathcal{N}(\\hat{y},\\,\\sigma^2)\\] 3.4.2 Homoskedasticity In the same way, the residuals must be homoscedastic. That is, the error does not change much for different values of the explanatory variables. All residuals \\(\\epsilon\\) follow the same distribution, the variance \\(\\sigma^2\\) remains constant. 3.4.3 Independence of the residuals It is also essential that the residuals are independent, i.e. that there is no missing structure in the model (such as spatial or temporal autocorrelation). In other words, each residual \\(\\epsilon_i\\) is independent of any other residual. 3.5 Notation for linear models This section aims to provide a basic understanding of the different types of linear model descriptions that can be encountered. The first is mathematical notation and the second is R notation. 3.5.1 Mathematical notation Mathematical notation is the only form of notation appropriate for manuscript writing. We note individual observations using the equation of the line as seen in the previous sections and the equation for the distribution of residuals: \\[y_i = \\beta_0 + \\beta_1 \\times x_i + \\epsilon_i \\quad \\textrm{with} \\quad \\epsilon_i \\sim \\mathcal{N}(0,\\,\\sigma^2)\\] To write down the set of observations, we follow the matrix notation and include the intercept in$\\(mathbf{X}\\) and \\(\\boldsymbol{\\beta}\\)) : \\[\\mathbf{y}= \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon} \\quad \\textrm{with} \\quad \\epsilon_i \\sim \\mathcal{N}(0,\\,I_n\\sigma^2)\\] 3.5.2 R notation Note that the R notation is not adequate for preparing a publication. In R, we write the model formula as : y ~ 1 + x Or simply as: y ~ x which also includes the constant. The predicted variable is always placed to the left of the ~ tilde, while the predictor variables are placed to the right. You should never mix the different types of notation! 3.6 Fitting a linear model We have seen that it is possible to define a linear model with this equation : \\[ y_i = \\beta_0 + \\beta_1 \\times x_i + \\epsilon_i\\] 3.6.1 Model estimation To run a linear model is to find the “best” estimates of the parameters \\(\\beta_0,\\, \\beta_1\\). This is what we call doing the estimation of the model. The “best” parameters are those that minimize the variation in the response variable. The most common method of minimizing the variation in a model is to sum the residuals squared \\(\\sum{epsilon_i^2}\\). This method is called the ordinary least squares (OLS) method. 3.7 Learning objectives "],["linear-regression-in-r.html", "Chapter 4 Linear regression in R 4.1 Model formulation 4.2 Linear regression in R 4.3 Model interpretation 4.4 Challenge 2 4.5 Linear regression in R 4.6 Variable names", " Chapter 4 Linear regression in R In the following example, we will take the bird abundance and mass data and run a linear regression in R. Based on the hypothesis, we will formulate an equation that allows us to run the model and then check the assumptions under which the model can be run. We will also see how to interpret the output of the model and how to represent it graphically. 4.1 Model formulation The data are composed of the average mass of individuals of a species and the abundance of those species. We are interested in quantifying the relationship between the mass of an individual and the abundance of the species (or the effect of mass on abundance). We hypothesized that for different bird species, the average mass of an individual has an effect on the maximum abundance of the species, due to ecological constraints (food sources, habitat availability, etc.). 4.1.1 Model equation In mathematical notation, a linear model takes the form of the equation of a line for which the predicted variable is the maximum abundance “MaxAbund” and the predictor variable is the mass of individuals “Mass”. \\[\\textrm{MaxAbund}_i = \\beta_0 + \\beta_1 \\times \\textrm{Mass}_i + \\epsilon_i \\;, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\] Note that the linear model has three variables: \\(\\beta_0\\) , \\(\\beta_1\\) and \\(\\sigma^1\\). \\(\\sigma^1\\) defines the variance of the data around the model and is a variable of the normal distribution that describes the distribution of the data around the model (on either side of our line). Model formula in R In the R programming language, the equation is translated as : MaxAbund ~ Mass where the predicted variable is placed on the left of the tilde and the predictor variable is on the right. 4.2 Linear regression in R Performing a linear regression with R is divided into three steps: Formulate and run a linear model based on a hypothesis Check the conditions for the linear model Examine the output of the model and whether the conditions are met Analyze the regression parameters Plot the model Perform significance tests on the parameter estimates (if necessary) We will explore each step in the following sections. Also, in the case where the conditions are not met, we will see that it is possible to consider the use of a Generalized Linear Model (GLM) or the transformation of the data. 4.2.1 Step 1: Formulate and run a linear model The lm() command is used to fit a linear model where the first argument is the model formula: # Linear regression of maximum abundance against mass lm1 &lt;- lm(MaxAbund ~ Mass, data = bird) With this line of code, we define a new lm1 object that contains the linear model. We also specify two arguments to the function. The first MaxAbund ~ Mass is the model formula and the second bird defines the object that contains the variables. Before using a new function in R, you should refer to its help page (?functionname) to understand how to use the function and the default parameters. Let’s look at the parameter estimates: # Examination of the regression output lm1 ## ## Call: ## lm(formula = MaxAbund ~ Mass, data = bird) ## ## Coefficients: ## (Intercept) Mass ## 38.16646 0.01439 How do the parameters compare to our predictions? You will notice that the parameter for mass is positive while we predicted a negative relationship (Species characterized by larger individuals have lower maximum abundance). However, can we trust the estimates? To be sure, we need to check the application conditions! 4.2.2 Step 2: Verify assumptions using diagnostic plots of the residuals An effective method of verifying that the model’s assumptions are met is to conduct a visual examination. Four diagnostic plots can be produced from an lm object. To do this, we use these commands: # Plot the four diagnostic plots par(mfrow = c(2, 2)) plot(lm1) par() is a function that allows you to define the parameters of the graph. Here we specify mfrow=c(2,2) which displays a grid of 2 x 2 graphs at a time. Finally, plot() produces the graphics. To display only one plot at a time, we can specify par(mfrow=1). 4.2.2.1 Diagnostic plot # 1 - Residuals vs Fitted The first plot informs us of the distribution of the residuals according to the values predicted by the linear regression model. Each point represents the distance between the response variable and the model prediction. It informs us about the independence of the residuals and their distribution. Remember that with linear regression, we need a uniform distribution of the residuals (the homoscedasticity condition). On the y axis we find the residuals \\(\\epsilon_i\\) and on the x axis the predicted values \\(\\hat{y_i} = \\beta_0 + \\beta_1 \\times x_i\\). In an ideal situation, there is no pattern to the dispersion of the points. If the residuals are scattered randomly around the line of 0, the relationship is linear and the mean of the residuals is 0. If the residuals form an approximate horizontal band around the 0 line, the variance of the residuals is homogeneous (i.e. they are homoscedastic). If the residuals are organized in a funnel shape, the residuals are not homoscedastic. Warning! You should be alerted if the distribution of the points is non-linear as in these two examples: These examples present a non-linear relationship and an example of heteroscedasticity which is the opposite of homoscedasticity, meaning that the normality condition is not met. In this situation, one should rather use a generalized linear model (GLM) that allows for other distributions (Poisson, binomial, negative binomial, etc.) or try to transform the response variable and/or the predictors. 4.2.2.2 Diagnostic plot # 2 - Scale Location The second diagnostic graph allows us to check if the dispersion of the residuals is increasing for a given predicted value (i.e. if the dispersion of the residuals is caused by the explanatory variable). If the dispersion increases, the basic condition of homoscedasticity is not respected. On the y axis we find the square root of the standardized residuals \\(\\sqrt{\\frac{\\epsilon_i}{\\sigma}}\\) and on the x axis the predicted values \\(\\hat{y_i} = \\beta_0 + \\beta_1 \\times x_i\\). Here we are also looking for a dispersion of the points without a pattern, thus an evenly distributed predictor. Caution! Be careful if the distribution of the points shows a marked trend: In a situation like this, we cannot rely on model results and should instead try to use a generalized linear model (GLM) that allows for other distributions (Poisson, binomial, negative binomial, etc.) or transform the response variable and/or the predictors. 4.2.2.3 Diagnostic plot # 3 - Normal QQ The third plot shows the distribution of the residuals. With this quantile-quantile plot, we can evaluate the normality of the residuals. This plot compares the probability distribution of the residuals of the model to a probability distribution of normal data. On the y axis we find the standardized residuals \\(\\frac{epsilon_i}{\\sigma}\\) and on the x axis the quantiles of a standard normal distribution \\(\\mathcal{N}(0, \\sigma^2)\\). We want to see the standardized residuals are near the 1:1 line. Thus, the residuals can be considered normally distributed. Caution! Be suspicious if the distribution of the points does not follow the 1:1 line: In this case, the points are not well aligned on the line, suggesting that the residuals are not normally distributed. Instead, try using a generalized linear model (GLM) that allows for other distributions (Poisson, binomial, negative binomial, etc.) or transform the response variable and/or the predictors. 4.2.2.4 Diagnostic plot # 4 - Residuals vs Leverage This last diagnostic plot shows the residuals and their influence. It allows us to determine if certain observations have a strong influence. Although we are not testing for a baseline condition, the presence of points with a strong influence can influence our interpretation of the data. If one or more observations are outliers (i.e., if they have very different values from the others), the model may be misfitted because of their exaggerated influence on the model estimate. The residuals vs. leverage plot shows the leverage points which are extreme observations of the predictor and their influence on the regression. The influence is quantified by the Cook distance. A distance greater than 0.5 is problematic. Example of leverage and influence. These are not diagnostic plots, but figures that illustrate the concepts of leverage and influence. We want to observe residuals that fall within the dotted lines marking the Cook distance of 0.5. Caution! Beware if a point or points lie outside the dotted line: The 29 has leverage and a Cook’s distance of more than 0.5. It corresponds to an outlier. However, outliers should never be removed without good reason to do so! If (and only if!) these observations are measurement errors or exceptions, they can be removed from the data set. 4.2.3 Step 2. Verify assumptions of lm1 Does lm1 violate any assumptions of the linear model? Based on the insights gained in the previous sections, there are serious problems with these diagnostic plots. * Plots 1 and 2 shows strong trends, * Plot 3 shows that the residuals do not follow a normal distribution, * Plot 4 highlights the leverage of point 32 and its very high influence. 4.2.4 Assumptions not met - what is wrong? To understand the source of the problems with the linear model, let’s plot the model with the observations: # Plot linear model and observations par(mfrow = c(1, 2)) coef(lm1) # constant and slope ## (Intercept) Mass ## 38.16645523 0.01438562 plot(MaxAbund ~ Mass, data = bird) # left plot abline(lm1) # line defined by the model parameters hist(residuals(lm1)) # plot on the right : distribution of residuals On the left we have the data as well as the line representing the model estimation (MaxAbund ~ Mass). On the left, the distribution of the residuals. We can check if the residuals follow a normal distribution using a Shapiro-Wilk test and a skewness test (skewness): # Test the normality of residuals shapiro.test(residuals(lm1)) ## ## Shapiro-Wilk normality test ## ## data: residuals(lm1) ## W = 0.64158, p-value = 3.172e-10 # Skewness test library(e1071) skewness(residuals(lm1)) ## [1] 3.154719 The Shapiro-Wilk test compares the distribution of the observed data to a normal distribution. The observed data can be considered normally distributed when the p-value calculated by the Shapiro-Wilk test is greater than the α significance level (usually 0.05). Here, the p-value (p-value) returned indicates a distribution that is significantly different from normal. The skewness test measures the shift of the residuals. A positive value represents a leftward shift and a negative value a rightward shift. Here, the positive skewness value indicates that the distribution is left-shifted. 4.2.5 Assumptions not met - how to proceed? Clearly, the assumptions of the linear model lm1 are violated. Therefore, we cannot rely on the results of the model. We must turn to another alternative and there are two options when the assumptions of the linear model are violated. **Use a different type of model QCBS R Workshops 6-8 explore other models that may be a better fit for the hypothesis and data. Transform the data It is also possible to modify one’s data, whether it be the response variable or the predictors. Several types of transformations are possible and their usefulness depends on the distribution of the variable and the type of model. However, modifying variables is often tricky in practice. The transformation may solve some problems, but may create other. Note that results of statistical tests on transformed data do not automatically hold for the untransformed data. 4.2.5.1 Challenge 1: A model on transformed variables Our model has some serious problems. Let’s try a logarithmic transformation to see if it can correct the situation. First, transform the data and run a new linear regression on it. Then, verify the assumptions of model using diagnostic plots. Step 1 Let’s start by transforming the data in our dataset: # log-transform the variables bird$logMaxAbund &lt;- log10(bird$MaxAbund) bird$logMass &lt;- log10(bird$Mass) With these logMaxAbund and logMass data transformed, we run a new linear regression and save the model object as lm2. # Linear model with transformed data lm2 &lt;- lm(logMaxAbund ~ logMass, data = bird) We can observe that the parameters align with our prediction. That is, the logMass coefficient is negative and therefore the abundance decreases with the mass of the species. However, we still need to check the assumptions of the model to ensure the validity of the results. Step 2 Let’s check the assumptions for lm2 using the diagnostic plots. # Diagnostic plots par(mfrow = c(2, 2), mar = c(3, 4, 1.15, 1.2)) plot(lm2) The diagnostic plots look much better, but there are still problems. Some plots still show strong trends. Let’s plot the model with the observations: # Plot linear model and observations par(mfrow = c(1, 2)) coef(lm2) ## (Intercept) logMass ## 1.6723673 -0.2361498 plot(logMaxAbund ~ logMass, data = bird) abline(lm2) hist(residuals(lm2)) The model appears to explain the data better and the distribution of residuals has moved much closer to the normal distribution. 4.2.6 Step 3. Analyze parameter estimates The last step in a linear regression with R is to check the model parameters. We then use the summary() command to get more information about the fitted model. # Print fitted model parameters summary(lm2) Call: lm(formula = logMaxAbund ~ logMass, data = bird) Residuals: Min 1Q Median 3Q Max -1.93562 -0.39982 0.05487 0.40625 1.61469 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.6724 0.2472 6.767 1.17e-08 *** logMass -0.2361 0.1170 -2.019 0.0487 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.6959 on 52 degrees of freedom Multiple R-squared: 0.07267, Adjusted R-squared: 0.05484 F-statistic: 4.075 on 1 and 52 DF, p-value: 0.04869 The output of summary() contains several values: 1. Coefficients: Estimates of the parameters and their standard deviation 2. Pr(&gt;|t|): Results of a t-test to determine if the parameters are different from 0 3. Adjusted R squared: How well does the model explain the data? 4. F-statistic(ANOVA): Is the model significantly different from a model with no predictor (null model)? We will discuss T-tests and ANOVA in the next section. However, we can already see that our model is only marginally better than the null model. With the lm2 object, we can also extract the parameters of the model and the other results: # Vectors of residuals and predicted values e &lt;- residuals(lm2) y &lt;- fitted(lm2) coefficients(lm2) # coefficients ## (Intercept) logMass ## 1.6723673 -0.2361498 summary(lm2)$coefficients # coefficients and T-tests ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.6723673 0.2471519 6.766557 1.166186e-08 ## logMass -0.2361498 0.1169836 -2.018658 4.869342e-02 summary(lm2)$adj.r.squared # adjusted R squared ## [1] 0.05483696 4.3 Model interpretation Model interpretation is the final step when we obtain a model that meets the assumptions. Interpretation is the evaluation of the model’s support for the hypothesis. In short, we answer the question: To what extent does the model support our hypothesis? Hypothesis For different bird species, the average mass of an individual affects the maximum abundance of the species, due to ecological constraints (food sources, habitat availability, etc.). # Summary of the linear model with log-transformed data summary(lm2) Call: lm(formula = logMaxAbund ~ logMass, data = bird) Residuals: Min 1Q Median 3Q Max -1.93562 -0.39982 0.05487 0.40625 1.61469 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.6724 0.2472 6.767 1.17e-08 *** logMass -0.2361 0.1170 -2.019 0.0487 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.6959 on 52 degrees of freedom Multiple R-squared: 0.07267, Adjusted R-squared: 0.05484 F-statistic: 4.075 on 1 and 52 DF, p-value: 0.04869 The lm2 model has very little evidence to support the hypothesis. Looking at the parameters of the model as discussed above, we observe that the model explains little of the response. The low Adjusted R squared indicates that little of the variance in the response variable is explained by the model. Mass explains only a small fraction of the species abundance. The F-test is barely significant indicating that the model is only slightly better than a model without predictor variables. Finally, the estimated effect size (the coefficient) of the parameter logMass is very close to zero and is only marginally significant, i.e. an increase or decrease in species mass causes almost no change in their abundance. The results are not as good as we would like, perhaps we should formulate a more precise hypothesis? 4.3.1 Finding a better model: terrestrial birds Let us formulate a new and more precise hypothesis. This time let’s focus only on land birds. Waterfowl abundance may not respond to mass in the same way as land birds. New hypothesis For different bird species terrestrial, the average mass of an individual has an effect on the maximum abundance of the species, due to ecological constraints (food sources, habitat availability, etc.). We can now fit the linear model to exclude waterfowl: # Linear model with terrestrial birds lm3 &lt;- lm(logMaxAbund ~ logMass, data = bird, subset = !bird$Aquatic) # excludes waterfowls (!birdsAquatic == TRUE) or in an # equivalent way: lm3 &lt;- lm(logMaxAbund~logMass, data=bird, # subset=bird$Aquatic == 0) # Model output lm3 ## ## Call: ## lm(formula = logMaxAbund ~ logMass, data = bird, subset = !bird$Aquatic) ## ## Coefficients: ## (Intercept) logMass ## 2.2701 -0.6429 From the diagnostic plots, we can conclude that the application conditions are met! Finally, the model provides evidence to support our hypothesis: summary(lm3) Call: lm(formula = logMaxAbund ~ logMass, data = bird, subset = !bird$Aquatic) Residuals: Min 1Q Median 3Q Max -1.78289 -0.23135 0.04031 0.22932 1.68109 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.2701 0.2931 7.744 2.96e-09 *** logMass -0.6429 0.1746 -3.683 0.000733 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.6094 on 37 degrees of freedom Multiple R-squared: 0.2682, Adjusted R-squared: 0.2485 F-statistic: 13.56 on 1 and 37 DF, p-value: 0.000733 The Adjusted R-squared is 0.25, indicating that the model is a reasonably good fit to the data. The model is much better than a model without predictor variables. The F-test is significant (&lt; 0.05). The T-test shows that the estimate of the parameter “logMass” is clearly different from 0 and that mass has an effect on species abundance. 4.4 Challenge 2 For the second challenge, let’s put all the steps together and try with new data: Formulate another similar hypothesis about the maximum abundance and average mass of an individual, this time for passerine birds. Fit a model to evaluate this hypothesis, using the transformed variables (i.e. logMaxAbund and logMass). Save the model as lm4. Verify assumptions of the linear model using the diagnostic plots. Interpret the results: Does the model provide evidence to support the hypothesis? Hint: Like aquatic species, passerines (variable Passerine) are coded 0/1 (check with str(bird)) 4.4.1 Solutions Hypothesis For different species of passers, the average mass of an individual affects the maximum abundance of the species, due to ecological constraints (food sources, habitat availability, etc.). Fitting the model # Fitting a linear model on passerine birds lm4 &lt;- lm(logMaxAbund ~ logMass, data = bird, subset = bird$Passerine == 1) lm4 ## ## Call: ## lm(formula = logMaxAbund ~ logMass, data = bird, subset = bird$Passerine == ## 1) ## ## Coefficients: ## (Intercept) logMass ## 1.2429 0.2107 Verify assumptions of the linear model par(mfrow = c(2, 2)) plot(lm4) Is it worth interpreting the results? summary(lm4) Call: lm(formula = logMaxAbund ~ logMass, data = bird, subset = bird$Passerine == 1) Residuals: Min 1Q Median 3Q Max -1.24644 -0.20937 0.02494 0.25192 0.93624 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.2429 0.4163 2.985 0.00661 ** logMass 0.2107 0.3076 0.685 0.50010 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.5151 on 23 degrees of freedom Multiple R-squared: 0.02, Adjusted R-squared: -0.02261 F-statistic: 0.4694 on 1 and 23 DF, p-value: 0.5001 The results of the model should not be interpreted, because the assumptions of the linear model are not met! 4.5 Linear regression in R Performing a linear regression with R is divided into three steps: Formulate and run a linear model based on a hypothesis Check the assumptions for the linear model Examine the output of the model and whether the conditions are met Analyze the regression parameters Plot the model Perform significance tests on the parameter estimates (if necessary) We will explore each step in the following sections. Also, in the case where the assumptions are not met, we will see that it is possible to consider the use of a Generalized Linear Model (GLM) or data transformation. 4.6 Variable names Different terms are used for the response and the predictor, depending on the context and the scientific field. The terms are not always synonymous. Here are two tables to help you understand the jargon you will encounter. response predictor explanatory var. covariate outcome output var. input var. dependent var. independent var. "],["t-test-and-anova.html", "Chapter 5 t-test and ANOVA 5.1 Analysis of Variance (ANOVA) 5.2 Two-way ANOVA 5.3 Unbalanced ANOVA (advanced section/ optional)", " Chapter 5 t-test and ANOVA 5.1 Analysis of Variance (ANOVA) Analysis of Variance (ANOVA) is a type of linear model for a continuous response variable and one or more categorical explanatory variables. The categorical explanatory variables can have any number of levels (groups). For example, the variable \"colour\" might have three levels: green, blue, and yellow. ANOVA tests whether the means of the response variable differ between the levels by comparing the variation within a group with the variation amon groups. For example, if blueberries differ in their mass depending on their colour. ANOVA calculations are based on the sum of squares partitioning and compares the within-level variance to the between-level variance. If the between-level variance is greater than the within-level variance, this means that the levels affect the explanatory variable more than the random error (corresponding to the within-level variance), and that the explanatory variable is likely to be significantly influenced by the levels. In the ANOVA, the comparison of the between-level variance to the within-level variance is made through the calculation of the F-statistic that correspond to the ratio of the mean sum of squares of the level (MSLev) on the mean sum of squares of the error (MS\\(\\epsilon\\)). These two last terms are obtained by dividing their two respective sums of squares by their corresponding degrees of freedom, as is typically presented in a ANOVA table . Finally, the p-value of the ANOVA is calculated from the F-statistic that follows a Chi-square (χ2) distribution. Source of variation Degrees of freedom (df) Sums of squares Mean squares F-statistic Total \\(ra-1\\) \\(SS_{t}=\\sum(y_{i}-\\overline{y})^{2}\\) Facteur A \\(a-1\\) \\(SS_{f}=\\sum(\\hat{y_{i}}-\\overline{y})^{2}\\) \\(MS_{f}=\\frac{SS_{f}}{(a-1)}\\) \\(F=\\frac{MS_{f}} {MS_{E}}\\) Error \\(a(r-1)\\) \\(SS_{\\epsilon}=\\sum(y_{i}-\\hat{y_{i}})^{2}\\) \\(MS_{\\epsilon}=\\frac{SS_{\\epsilon}}{a(r-1)}\\) \\(a\\): number of levels of the explanatory variable A \\(r\\): number of replicates per level \\(\\overline{y}\\): general mean of the explanatory variable \\(\\hat{y_{i}}\\) : mean of the explanatory variable for all the replicates of the level i. 5.1.1 Types of ANOVA One-way ANOVA One categorical explanatory variable with 2 or more levels. If there are 2 levels a t-testcan be used alternatively. Two-way ANOVA (see section 4.1.3) 2 categorical explanatory variables or more, Each categorical explanatory variable can have multiple levels, The interactions between each categorical explanatory variable can be tested. Repeated measures ANOVA can be used for repeated measures, but we won’t cover this today. Linear Mixed-effect Models can also be used for this kind of data (see Workshop 6). 5.1.2 T-test When you have a single explanatory variable which is qualitative and only have two levels, you can run a student’s T-test to test for a difference in the mean of the two levels. If appropriate for your data, you can choose to test a unilateral hypothesis. This means that you can test the more specific assumption that one level has a higher mean than the other, rather than that they simply have different means.Note that robustness of this test increases with sample size and is higher when groups have equal sizes For the t-test, the t statistic used to find the p-value calculation is calculated as: \\(t = (\\overline{y_{1}}-\\overline{y_{2}})/\\sqrt{\\frac{s_{1}^2} n_{1} + \\frac{s_{2}^2} n_{2}}\\) where \\(\\overline{y_{1}}\\) and \\(\\overline{y_{2}}\\) are the means of the response variable y for group 1 and 2, respectively, \\(s_{1}^2\\) and \\(s_{2}^2\\) are the variances of the response variable y for group 1 and 2, respectively, \\(n_{1}\\) and \\(n_{2}\\) are the sample sizes of groups 1 and 2, respectively. Note that the t-test is mathematically equivalent to a one-way ANOVA with 2 levels. 5.1.2.1 Assumptions If the assumptions of the t-test are not met, the test can give misleading results. Here are some important things to note when testing the assumptions of a t-test. Normality of data As with simple linear regression, the residuals need to be normally distributed. If the data are not normally distributed, but have reasonably symmetrical distributions, a mean which is close to the centre of the distribution, and only one mode (highest point in the frequency histogram) then a t-test will still work as long as the sample is sufficiently large (rule of thumb ~30 observations). If the data is heavily skewed, then we may need a very large sample before a t-test works. In such cases, an alternate non-parametric test should be used. Homoscedasticity Another important assumption of the two-sample t-test is that the variance of your two samples are equal. This allows you to calculate a pooled variance, which in turn is used to calculate the standard error. If population variances are unequal, then the probability of a Type I error is greater than α. The robustness of the t-test increases with sample size and is higher when groups have equal sizes. We can test for difference in variances among two populations and ask what is the probability of taking two samples from two populations having identical variances and have the two sample variances be as different as are \\(s_{1}^2\\) and \\(s_{2}^2\\). To do so, we must do the variance ratio test (i.e. an F-test). 5.1.2.2 Violation of assumptions If variances between groups are not equal, it is possible to use corrections, like the Welch correction. If assumptions cannot be respected, you can transform your data (log or square root for example) or use the non-parametric equivalent of t-test, the Mann-Whitney test. Finally, if the two groups are not independent (e.g. measurements on the same individual at 2 different years), you should use a Paired t-test. 5.1.2.3 Running a t-test In R, t-tests are implemented using the function t.test. t.test(Y ~ X2, data = data, alternative = &quot;two.sided&quot;, var.equal = TRUE) where: -Y is the response variable -X2 is a qualitative variable with two levels -data is the name given to your data set -alternative refers to the alternative hypothesis. 'two.sided' is the default setting. You can also choose 'less' or 'greater'. For more details, look this section. -var.eqal, if true, this increases the robustness of the test. But we need to test for it prior to testing. Let’s look at an exemple with bird diet data. For example, let’s test the mass difference between aquatic and non-aquatic birds qith the column Aquatic. First, we can start with visualizing the data: There seem to be a difference just by looking at the boxplot. Let’s make sure are assumptions are met before doing the test. Note: we do not need to test the assumption of normally distributed data since we already log transformed the data above # Assumption of equal variance var.test(logMass ~ Aquatic, data = bird) ## ## F test to compare two variances ## ## data: logMass by Aquatic ## F = 1.0725, num df = 38, denom df = 14, p-value = 0.9305 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.3996428 2.3941032 ## sample estimates: ## ratio of variances ## 1.072452 Here, we show that the ratio of variances is not statistically different from 1, therefore variances are equal, and we proceeded with our t-test. # We are now ready to run the t-test ttest1 &lt;- t.test(x = bird$logMass[bird$Aquatic == 0], y = bird$logMass[bird$Aquatic == 1], var.equal = TRUE) # or equivalently ttest1 &lt;- t.test(logMass ~ Aquatic, var.equal = TRUE, data = bird) ttest1 ## ## Two Sample t-test ## ## data: logMass by Aquatic ## t = -7.7707, df = 52, p-value = 2.936e-10 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -3.838340 -2.262829 ## sample estimates: ## mean in group 0 mean in group 1 ## 3.645998 6.696582 Since p &lt; 0.05, the hypothesis of no difference between the two bird types (Aquatic vs. terrestrial) was rejected. 5.1.2.4 Running a t-test with lm() Do not forget that a t test is simply a linear model or an ANOVA for a factor with two levels. You can reach the same results with : lm.t &lt;- lm(logMass ~ Aquatic, data = bird) anova(lm.t) ## Analysis of Variance Table ## ## Response: logMass ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Aquatic 1 100.816 100.82 60.385 2.936e-10 *** ## Residuals 52 86.817 1.67 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 When variances are equal (i.e., two-sample t-test), we can show that t2 = F: ttest1$statistic^2 ## t ## 60.3845 anova(lm.t)$F ## [1] 60.3845 NA 5.1.2.5 Unilateral t-test The alternative option of the t.test function allows for the use of unilateral t-test. By default, t.test() use 'two-sided', which tests whether the means are different from each other. If you want to test whether one mean is higher than the other one, you can use 'less' (\\(\\overline{y_{1}}\\) &lt; \\(\\overline{y_{2}}\\)) or 'greater' (\\(\\overline{y_{1}}\\) &gt; \\(\\overline{y_{2}}\\)). For example, if users want to test if non-aquatic birds are less heavy than aquatic birds, the function can be written: # Unilateral T-test uni.ttest1 &lt;- t.test(logMass ~ Aquatic, var.equal = TRUE, data = bird, alternative = &quot;less&quot;) In the R output, called by uni.ttest1, the results of the t-test appear in the third line: ## ## Two Sample t-test ## ## data: logMass by Aquatic ## t = -7.7707, df = 52, p-value = 1.468e-10 ## alternative hypothesis: true difference in means between group 0 and group 1 is less than 0 ## 95 percent confidence interval: ## -Inf -2.393147 ## sample estimates: ## mean in group 0 mean in group 1 ## 3.645998 6.696582 In this case, the calculated t-statistic is t = -7.7707 with df = 52 degrees of freedom that gives a p-value of p-value = 1.468e-10. As the calculated p-value is inferior to 0.05, the null hypothesis is rejected. Thus, aquatic birds are significantly heavier than non-aquatic birds. We could have tested whether non-aquatic birds were heavier than aquatic bird. By testing the opposite, we get the same t value (but negative) and a p-value reflected accordingly. We can therefore conclude that non-aquatic birds are not significantly heavier than aquatic birds. # Test de t en spécifiant l&#39;argument &#39;alternative&#39; uni.ttest1 &lt;- t.test(logMass ~ Aquatic, var.equal = TRUE, data = bird, alternative = &quot;greater&quot;) uni.ttest1 ## ## Two Sample t-test ## ## data: logMass by Aquatic ## t = -7.7707, df = 52, p-value = 1 ## alternative hypothesis: true difference in means between group 0 and group 1 is greater than 0 ## 95 percent confidence interval: ## -3.708022 Inf ## sample estimates: ## mean in group 0 mean in group 1 ## 3.645998 6.696582 5.1.3 Running an ANOVA The t-test is only for a single categorical explanatory variable with 2 levels. For all other linear models with categorical explanatory variables we use ANOVA. When the ANOVA detects a significant difference between groups, it does not tell you which group (or groups) differs from the others.A commonly used post-hoc test to answer this question is the Tukey’s test. You may also compare between groups using planned comparisons or contrasts. This is more elegant, because it expects that you have an a priori expectation for the differences between groups. 5.1.4 Verifying assumptions As with the simple linear regression and t-test, ANOVA must meet the four assumptions of linear models. Below are some tips in how to test these assumptions for an ANOVA. Normal distribution The residuals of ANOVA model can once again be visualised in the normal QQplot. If the residuals lie linearly on the 1:1 line of the QQplot, they can be considered as normally distributed. If not, the ANOVA results cannot be interpreted. Homoscedasticity To be valid, ANOVA must be performed on models with homogeneous variance of the residuals. This homoscedasticity can be verified using either the residuals vs fitted plot or the scale-location plot of the diagnostic plots. If these plots present equivalent spread of the residuals for each of the fitted values, then the residuals variance can be considered homogeneous. A second way to assess the homogeneity of residuals variance is to perform a Bartlett test on the anova model using the function bartlett.test. If the p-value of this test is superior to 0.05, the null hypothesis H0: s12 = s22 =... = sj2 =... = sn2 is accepted and the homoscedasticity assumption is respected. Usual transformations of explanatory variables can be used if the homogeneity of residuals variance is not met. Additivity In addition to the assumption testing, it is important to consider whether the effects of two factors are additive. The effects are additive if the effect of one factor remains constant over all levels of the other factor, and that each factor influences the response variable independently of the other factor(s). If assumptions are violated your can try to transform your data, which could potentially equalize variances and normalize residuals, and can convert a multiplicative effect into an additive effect. Or, if you can’t (or don’t want to) transform your data, the non-parametric equivalent of ANOVA is Kruskal-Wallis test. 5.1.4.1 Example with bird data We are interested to test if maximal abundance of birds depend on the diet. First, let’s visualize the data using boxplot(). Recall that by default, R will order you groups in alphabetical order. We can reorder the groups according to the median of each Diet level. # Default alphabetical order boxplot(logMaxAbund ~ Diet, data = bird) # Relevel factors med &lt;- sort(tapply(bird$logMaxAbund, bird$Diet, median)) boxplot(logMaxAbund ~ factor(Diet, levels = names(med)), data = bird, col = c(&quot;white&quot;, &quot;lightblue1&quot;, &quot;skyblue1&quot;, &quot;skyblue3&quot;, &quot;skyblue4&quot;)) Another way to graphically view the effect sizes is to use plot.design(). This function will illustrate the levels of a particular factor along a vertical line, and the overall value of the response is drawn as a horizontal line. plot.design(logMaxAbund ~ Diet, data = bird, ylab = expression(&quot;log&quot;[10] * &quot;(Maximum Abundance)&quot;)) There seem to be some differences. We now need to check the assumption to our model. Note: We need to perform the ANOVA to look at the residuals, however, results are not to be interpreted until we have verified the assumptions. # Plot for diagnostics aov1 &lt;- aov(logMaxAbund ~ Diet, data = bird) opar &lt;- par(mfrow = c(2, 2)) plot(aov1) par(opar) Ideally the first diagnostic plot should show similar scatter for each Diet level. We can use only these graph to verify our assumptions, however, we can also use the Shapiro’s, Bartlett’s tests or Levene’s test. Note:Levene’s test performs better, but has a slightly higher Type II error. # Test assumption of normality of residuals shapiro.test(resid(aov1)) ## ## Shapiro-Wilk normality test ## ## data: resid(aov1) ## W = 0.97995, p-value = 0.4982 # Test assumption of homogeneity of variance Bartlett&#39;s # test bartlett.test(logMaxAbund ~ Diet, data = bird) ## ## Bartlett test of homogeneity of variances ## ## data: logMaxAbund by Diet ## Bartlett&#39;s K-squared = 7.4728, df = 4, p-value = 0.1129 # Levene&#39;s test library(car) leveneTest(logMaxAbund ~ Diet, data = bird) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 4 2.3493 0.06717 . ## 49 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The tests are non-significant, therefore residuals are assumed to be normally distributed and variances are assumed to be equal. Let’s now run the ANOVA. In R, ANOVA can be called either directly with the aov function, or with the anova function performed on a linear model previously implemented with lm: # Using aov() aov1 &lt;- aov(logMaxAbund ~ Diet, data = bird) summary(aov1) # Using lm() anov1 &lt;- lm(logMaxAbund ~ Diet, data = bird) anova(anov1) 5.1.5 Model output Once your ANOVA model has been validated, its results can be interpreted. The R output of ANOVA model depends of the function that has been used to implement the ANOVA. If the aov function is used to implement the ANOVA model aov1 &lt;- aov(logMaxAbund ~ Diet, data = bird) the results of the ANOVA can be visualized using the function summary(aov1) On the other hand, if lm() is used anov1 &lt;- lm(logMaxAbund ~ Diet, data = bird) the ANOVA results must be called using the function anova(anov1) In both cases, the R output is as follows: ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Diet 4 27.07 6.768 2.836 0.0341 * ## Residuals 49 116.92 2.386 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This R output corresponds exactly to the ANOVA table of your model. This output also present the degrees of freedom, the sum of squares, the mean sum of squares and the F-value previously explained. For this example, the diet significantly influences the abundance of birds as the p-value is inferior to 0.05. The null hypothesis can then be rejected meaning that at least one of the diet treatments influenced the abundance differently than the other treatments. 5.1.6 Complementary test Importantly, ANOVA cannot identify which treatment is different from the others in terms of response variable. It can only identify that a difference is present. To determine the location of the difference(s), post-hoc tests that compare the levels of the explanatory variables (i.e. the treatments) two by two, must be performed. While several post-hoc tests exist (e.g. Fischer’s least significant difference, Duncan’s new multiple range test, Newman-Keuls method, Dunnett’s test, etc.), the Tukey’s range test is used in this example using the function TukeyHSD as follows: # Where does the Diet difference lie? TukeyHSD(aov(anov1), ordered = T) # or equivalently TukeyHSD(aov1, ordered = T) The R output for this test gives a table containing all the two by two comparisons of the explanatory variable levels and identify which treatment differ from the others: ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## factor levels have been ordered ## ## Fit: aov(formula = logMaxAbund ~ Diet, data = bird) ## ## $Diet ## diff lwr upr p adj ## Vertebrate-InsectVert 0.7746576 -2.56640638 4.115722 0.9645742 ## Insect-InsectVert 1.4815601 -1.76264078 4.725761 0.6965047 ## Plant-InsectVert 2.0364840 -2.33799553 6.410964 0.6812494 ## PlantInsect-InsectVert 2.4539424 -0.80660217 5.714487 0.2235587 ## Insect-Vertebrate 0.7069025 -0.89043156 2.304237 0.7204249 ## Plant-Vertebrate 1.2618265 -2.07923748 4.602890 0.8211024 ## PlantInsect-Vertebrate 1.6792848 0.04901254 3.309557 0.0405485 ## Plant-Insect 0.5549239 -2.68927692 3.799125 0.9884504 ## PlantInsect-Insect 0.9723823 -0.44885612 2.393621 0.3117612 ## PlantInsect-Plant 0.4174584 -2.84308619 3.678003 0.9961844 In this case, the only significant difference in abundance occurs between the PlantInsect diet and the Vertebrate diet. 5.1.7 Plotting After having verified the assumptions of your ANOVA model, interpreted the ANOVA table and differentiated the effect of the treatments using post-hoc tests or contrasts, the ANOVA results can be graphically illustrated using a barplot. This shows the response variable as a function of the explanatory variable levels, where standard errors can be superimposed on each bar as well as the different letters representing the treatment group (according to the post-hoc test). # Graphical illustration of ANOVA model using barplot() sd &lt;- tapply(bird$logMaxAbund, list(bird$Diet), sd) means &lt;- tapply(bird$logMaxAbund, list(bird$Diet), mean) n &lt;- length(bird$logMaxAbund) se &lt;- 1.96 * sd/sqrt(n) bp &lt;- barplot(means, col = c(&quot;white&quot;, &quot;lightblue1&quot;, &quot;skyblue1&quot;, &quot;skyblue3&quot;, &quot;skyblue4&quot;), ylab = expression(&quot;log&quot;[10] * &quot;(Maximum Abundance)&quot;), xlab = &quot;Diet&quot;, ylim = c(0, 5)) # Add vertical se bars segments(bp, means - se, bp, means + se, lwd = 2) # and horizontal lines segments(bp - 0.1, means - se, bp + 0.1, means - se, lwd = 2) segments(bp - 0.1, means + se, bp + 0.1, means + se, lwd = 2) # add a line at 0 abline(h = 0) 5.1.8 Going further: Contrasts Contrasts compare each level of a factor to a baseline level and therefore, we can determine if each level of a factor are significantly different from each other. Contrasts are based on an a priori hypothesis, which makes them much more robust than a posteriori comparison like Tukey’s test. With contrasts, groups can be compounded of one or many levels of a factor, and we can test basic hypothesis (ex: μ1 = μ2) or more complex hypothesis (ex: (μ1 + μ2)/3 == μ3). Important requirement The number of comparisons has to be lower or equal to the number of degrees of freedom of the ANOVA. Comparisons have to be independent from one another. For ANOVAs where explanatory variables are categorical, the intercept is the baseline group and corresponds to the mean of the first (alphabetically) level. By calculating the intercept and coefficient estimates of each Diet level, What do you notice? tapply(bird$logMaxAbund, bird$Diet, mean) ## Insect InsectVert Plant PlantInsect Vertebrate ## 2.656938 1.175378 3.211862 3.629321 1.950036 coef(anov1) ## (Intercept) DietInsectVert DietPlant DietPlantInsect DietVertebrate ## 2.6569384 -1.4815601 0.5549239 0.9723823 -0.7069025 coef(anov1)[1] + coef(anov1)[2] # InsectVert ## (Intercept) ## 1.175378 coef(anov1)[1] + coef(anov1)[3] # Plant ## (Intercept) ## 3.211862 Mean for Insect corresponds to the intercept of our ANOVA anov1. You can also reach those results by using the function summary.lm() or `summary(). This output performs a linear regression for each level of the explanatory variable and calculates their associated parameters. Note: When the ANOVA model was implemented with the aov function, this output is called using the function summary.lm(). Use the function summary() when the ANOVA was implemented with the lm function. ## ## Call: ## lm(formula = logMaxAbund ~ Diet, data = bird) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.2664 -0.7592 -0.2028 1.0908 3.5938 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.6569 0.3454 7.692 5.66e-10 *** ## DietInsectVert -1.4816 1.1456 -1.293 0.2020 ## DietPlant 0.5549 1.1456 0.484 0.6303 ## DietPlantInsect 0.9724 0.5019 1.938 0.0585 . ## DietVertebrate -0.7069 0.5640 -1.253 0.2161 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.545 on 49 degrees of freedom ## Multiple R-squared: 0.188, Adjusted R-squared: 0.1217 ## F-statistic: 2.836 on 4 and 49 DF, p-value: 0.0341 Users can note that the last line of this R output corresponds exactly to the previous R output shown in the ANOVA table. The F-statistic of the ANOVA model and its associated p-value (i.e. 2.836 and 0.0341), are the same one as the values obtained from the ANOVA table, indicating diet explains the abundance better than a null model, and so, diet significantly influence the abundance. The goodness-of-fit of the ANOVA model (i.e. adjusted R-square value) appears in the second to last line of this output. In this case, diet explains 12.17% of the abundance variability. Contrasts adjust a linear regression of the response variable as function of each level of the categorical explanatory variable separately. In this case, 5 linear regressions (corresponding to the five lines of the coefficients table of the R output) are calculated by the lm function as the diet variable contains 5 levels. By default, the baseline level corresponding to the intercept is the first level of explanatory variables ranked in alphabetical order. So in this case, the Insect diet is automatically used as a baseline in R. In the R output, the coefficient estimate of the baseline level (here, 1.1539) is first compared to 0 using a t-test (in this case, the t-test is significant with a p-value of 5.66e-10), while the coefficient estimates of the other explanatory variable level are compared to the baseline level. In this case, only the PlantInsect diet differs from the Insect diet, with an associated p-value of 0.0585. In other words, this R output allows us to determine the mean of the response variable for each of the diet levels, for example: LogMaxAbund = 1.1539 for the Insect diet, LogMaxAbund = 1.1539 – 0.6434 for the InsectVert diet, LogMaxAbund = 1.1539 + 0.2410 for the Plant diet, etc. As this type of contrasts compares each level of the explanatory variable to a baseline level, they are called contr.treatment and constitute the default method of the lm function in R. The baseline level can, however, be changed using the relevel function. For example, the following lines compare each diet treatment to the Plant diet, now defined as the baseline level. bird$Diet2 &lt;- relevel(bird$Diet, ref = &quot;Plant&quot;) anov_rl &lt;- lm(logMaxAbund ~ Diet2, data = bird) summary(anov_rl) ## ## Call: ## lm(formula = logMaxAbund ~ Diet2, data = bird) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.2664 -0.7592 -0.2028 1.0908 3.5938 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.2119 1.0923 2.941 0.00499 ** ## Diet2Insect -0.5549 1.1456 -0.484 0.63026 ## Diet2InsectVert -2.0365 1.5447 -1.318 0.19351 ## Diet2PlantInsect 0.4175 1.1513 0.363 0.71848 ## Diet2Vertebrate -1.2618 1.1798 -1.070 0.29006 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.545 on 49 degrees of freedom ## Multiple R-squared: 0.188, Adjusted R-squared: 0.1217 ## F-statistic: 2.836 on 4 and 49 DF, p-value: 0.0341 anova(anov_rl) ## Analysis of Variance Table ## ## Response: logMaxAbund ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Diet2 4 27.071 6.7677 2.8363 0.0341 * ## Residuals 49 116.918 2.3861 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You can also reorder multiple levels according to median, instead of the alphabetical order: med &lt;- sort(tapply(bird$logMaxAbund, bird$Diet, median)) bird$Diet2 &lt;- factor(bird$Diet, levels = names(med)) anov2 &lt;- lm(logMaxAbund ~ Diet2, data = bird) summary(anov2) ## ## Call: ## lm(formula = logMaxAbund ~ Diet2, data = bird) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.2664 -0.7592 -0.2028 1.0908 3.5938 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.1754 1.0923 1.076 0.2872 ## Diet2Vertebrate 0.7747 1.1798 0.657 0.5145 ## Diet2Insect 1.4816 1.1456 1.293 0.2020 ## Diet2Plant 2.0365 1.5447 1.318 0.1935 ## Diet2PlantInsect 2.4539 1.1513 2.131 0.0381 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.545 on 49 degrees of freedom ## Multiple R-squared: 0.188, Adjusted R-squared: 0.1217 ## F-statistic: 2.836 on 4 and 49 DF, p-value: 0.0341 anova(anov2) ## Analysis of Variance Table ## ## Response: logMaxAbund ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Diet2 4 27.071 6.7677 2.8363 0.0341 * ## Residuals 49 116.918 2.3861 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The contrasts coefficient matrix of these contr.treatment comparisons can be called by bird$Diet2 &lt;- relevel(bird$Diet, ref = &quot;Plant&quot;) contrasts(bird$Diet2) ## Insect InsectVert PlantInsect Vertebrate ## Plant 0 0 0 0 ## Insect 1 0 0 0 ## InsectVert 0 1 0 0 ## PlantInsect 0 0 1 0 ## Vertebrate 0 0 0 1 where each column corresponds to a comparison to the baseline Plant and each line to a diet level. For example, the first comparison compares the Insect diet to the Plant diet, the second one compares the InsectVert diet to the Plant diet, etc. The default contrasts contr.treatment is not orthogonal, meaning that the sum of the products of their coefficients is not null. We can verify if our previous contrasts are orthogonal by checking two things: - Coefficients must sum to 0 - Any two contrast columns must sum to 0 sum(contrasts(bird$Diet2)[, 1]) # first condition for column 1 ## [1] 1 sum(contrasts(bird$Diet2)[, 1] * contrasts(bird$Diet2)[, 2]) # second condition for column 1 and 2 ## [1] 0 The first column equal to 1, which mean our contrasts are not orthogonal. Orthogonality is important to make sure our contrasts are independant. You can use Helmert or polynomial contrasts to remedy the situation. Helmert contrast will contrast the second level with the first, the third with the average of the first two, and so on. options(contrasts = c(&quot;contr.helmert&quot;, &quot;contr.poly&quot;)) sum(contrasts(bird$Diet2)[, 1]) ## [1] 0 sum(contrasts(bird$Diet2)[, 1] * contrasts(bird$Diet2)[, 2]) ## [1] 0 Our conditions are now equal to 0 and our contrasts are orthogonal. anov3 &lt;- lm(logMaxAbund ~ Diet, data = bird) summary(anov3) ## ## Call: ## lm(formula = logMaxAbund ~ Diet, data = bird) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.2664 -0.7592 -0.2028 1.0908 3.5938 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.5247 0.3369 7.495 1.14e-09 *** ## Diet1 -0.7408 0.5728 -1.293 0.2020 ## Diet2 0.4319 0.4111 1.051 0.2986 ## Diet3 0.3203 0.1603 1.999 0.0512 . ## Diet4 -0.1437 0.1206 -1.191 0.2393 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.545 on 49 degrees of freedom ## Multiple R-squared: 0.188, Adjusted R-squared: 0.1217 ## F-statistic: 2.836 on 4 and 49 DF, p-value: 0.0341 Users can also create their own contrasts coefficient matrix in order to perform the comparison they desire using the contrasts function.For example, the following lines create this contrasts coefficient matrix: contrasts(bird$Diet2) &lt;- cbind(c(4, -1, -1, -1, -1), c(0, 1, 1, -1, -1), c(0, 0, 0, 1, -1), c(0, 1, -1, 0, 0)) contrasts(bird$Diet2) ## [,1] [,2] [,3] [,4] ## Plant 4 0 0 0 ## Insect -1 1 0 1 ## InsectVert -1 1 0 -1 ## PlantInsect -1 -1 1 0 ## Vertebrate -1 -1 -1 0 that compares: the Plant diet to all the other diets in the first comparison, the InsectVert and the Insect diets to the PlantInsect and the Vertebrate diets in the second one, the PlantInsect diet to the Vertebrate diet in the third one, and the Insect diet to the InsectVert diet in the fourth one. Thus, for each column, the treatments with identical contrasts coefficient belong to the same two by two comparison group (e.g. in the column 1, the four treatment with a -1 coefficient belong to the first comparison group and are altogether compare to the second group corresponding to the treatment with a different coefficient, here the Plant diet with a coefficient 4). Let’s verify the orthogonality of our contrasts: sum(contrasts(bird$Diet2)[, 1]) # first condition for column 1 ## [1] 0 sum(contrasts(bird$Diet2)[, 1] * contrasts(bird$Diet2)[, 2]) # second condition for column 1 and 2 ## [1] 0 These constrats are orthogonal and could be used to compare factor levels with each other. Other conventional contrast coefficient matrices are already programmed in R: help(contrasts) 5.2 Two-way ANOVA In the above section, the ANOVA models had a single categorical variable. We can create ANOVA models with multiple categorical explanatory variables. When there are two categorical explanatory variables, we refer to the model as a two-way ANOVA. A two-way ANOVA tests several hypotheses: that there is no difference in mean among levels of variable A; that there is no difference in mean among levels of variable B; and that there is no interaction between variables A and B. A significant interaction means the mean value of the response variable for each level of variable A changes depending on the level of B. For example, perhaps relationship between the colour of a fruit and its mass will depend on the plant species: if so, we say there is an interaction between colour and species. The one-way ANOVA table has to be rewritten to add the second explanatory term as well as the interaction term. Thus, a two-way ANOVA table corresponds to: Source of variation Degrees of freedom (df) Sums of squares Mean squares F-statistic Total \\(abr-1\\) \\(SS_{t}=\\sum_{i,j,k}{}(y_{ijk}-\\overline{y})^2\\) Cells \\(ab-1\\) \\(SS_{Cells}=\\sum_{i,j}(\\overline{y}_{ij}-\\overline{y})^2\\) Within- cells (error) \\(ab(r-1)\\) \\(SS_{\\epsilon}=\\sum_{i,j,k}(y_{ijk}-\\overline{y}_{ij})^2\\) \\(MS_{\\epsilon}=\\frac{SS_{\\epsilon}}{ab(r-1)}\\) Factor A \\(a-1\\) \\(SS_{A}= rb\\sum_{i}(\\overline{y}_{i.}-\\overline{y})^2\\) \\(MS_{A}=\\frac{SS_{A}}{a-1}\\) \\(F_{A}=\\frac{MS_{A}}{MS_{\\epsilon}}\\) Factor B \\(b-1\\) \\(SS_{B}= ra\\sum_{j}(\\overline{y}_{.j}-\\overline{y})^2\\) \\(MS_{B}=\\frac{SS_{B}}{b-1}\\) \\({F_B}=\\frac{MS_{B}}{MS_\\epsilon}\\) Interaction AB \\((a-1)(b-1)\\) \\(SS_{AB}= r\\sum_{i,j,k}(\\overline{y}_{..k}-\\overline{y}_{.jk}-\\overline{y}_{i.k})^2\\) \\(MS_{AB}=\\frac{SS_{AB}}{(a-1)(b-1)}\\) \\(F_{AB}=\\frac{MS_{AB}}{MS_\\epsilon}\\) a: number of levels of the explanatory variable A b: number of levels of the explanatory variable B r: number of replicates per treatment \\(\\epsilon\\) : error 5.2.1 4.1 Running a two-way ANOVA In R, a two-way ANOVA model is implemented in the same fashion as a one-way ANOVA using the function lm. One-way ANOVA aov &lt;- lm(Y ~ X, data) Two-way ANOVA aov &lt;- lm(Y ~ X1 * X2 * ..., data) The * symbol indicates that the main effects, as well, as their interaction will be included in the model. Always start reading the output from the interaction term, then proceed to the main effects. According to law of parsimony, select the model that explain the most variance with the least model parameters as possible: If the multiplicative effect (interaction) is non-signficant, you may consider a model with only the additive effects. You can then remove the interaction and run the model again. If use the + symbol, the main effects, but their interaction are not included. aov &lt;- lm(Y ~ X1 + X2 + ..., data) CHALLENGE 2 Examine the effects of the factors Diet, Aquatic, and their interaction on the maximum bird abundance. Recall: Before interpreting the ANOVA results, the model must first be validated by verifying the statistical assumptions of ANOVA, namely the: - Normal distribution of the model residuals Homoscedasticty of the residuals variance This verification can be done using the four diagnostic plots as previously explained for one-way ANOVA. Click to see the solution to Challenge 2! Assumptions anov4 &lt;- lm(logMaxAbund ~ Diet * Aquatic, data = bird) opar &lt;- par(mfrow = c(2, 2)) plot(anov4) ## Warning: not plotting observations with leverage one: ## 5, 6, 26 par(opar) Assumptions are met so we use the anova command to visualize the ANOVA table of the model: anova(anov4) ## Analysis of Variance Table ## ## Response: logMaxAbund ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Diet 4 27.071 6.7677 3.0378 0.02669 * ## Aquatic 1 1.688 1.6878 0.7576 0.38870 ## Diet:Aquatic 3 14.978 4.9926 2.2410 0.09644 . ## Residuals 45 100.252 2.2278 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In this case, the only significant term of the model is the diet factor as the p-value associated with the interaction term is not significant, meaning that the synergic effect of aquatic/non-aquatic type of birds does not influence the abundance. The significance of the interaction can also be determined by comparing two nested ANOVA models, i.e. a first model with the interaction and a second model without the interaction, using the anova command: anov5 &lt;- lm(logMaxAbund ~ Diet + Aquatic, data = bird) anova(anov5, anov4) ## Analysis of Variance Table ## ## Model 1: logMaxAbund ~ Diet + Aquatic ## Model 2: logMaxAbund ~ Diet * Aquatic ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 48 115.23 ## 2 45 100.25 3 14.978 2.241 0.09644 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 As the only difference between the two compared ANOVA models is the presence of the interaction term, this R output presents the significance of this term. In this case, the interaction term is not significant and can be dropped from the final ANOVA model. When the interaction term is significant, users should remember that the single effect of each explanatory variable cannot be interpreted and only the interaction term can. Note:The ANOVA table reports that the number of DF (degrees of freedom) for the interaction between Diet:Aquatic is 3. According to the notation in the two-way ANOVA table (for balanced designed), a = 5 and b = 2 and the DF for the interaction should be (a-1)(b-1) = 4*1 = 4. The R output provides a DF of 3 because the Diet*Aquatic treatment is extremely unbalanced. Namely, since there is no aquatic birds that feed on plants we loose that interaction (note the NA in the summary(anov4) output). Please refer to the advanced section on Unbalanced ANOVA below for further details. 5.2.2 4.2 Interaction plot Interactions can also be viewed graphically using the function interaction.plot as: interaction.plot(bird$Diet, bird$Aquatic, bird$logMaxAbund, col = &quot;black&quot;, ylab = expression(&quot;log&quot;[10] * &quot;(Maximum Abundance)&quot;), xlab = &quot;Diet&quot;) What do the gaps in the line for the Aquatic group mean? table(bird$Diet, bird$Aquatic) ## ## 0 1 ## Insect 14 6 ## InsectVert 1 1 ## Plant 2 0 ## PlantInsect 17 1 ## Vertebrate 5 7 The design is unbalanced; unequal observations among diet levels for Aquatic (coded as 1) and Terrestrial (coded as 0). See advanced section below for details on unbalanced ANOVA designs. CHALLENGE 3 Test the significance of the Aquatic factor by comparing nested models with and without this categorical variable. Cliquez pour voir la solution au Défi 3! anova(anov1, anov5) # Recall: anov1 is model with only Diet as a factor ## Analysis of Variance Table ## ## Model 1: logMaxAbund ~ Diet ## Model 2: logMaxAbund ~ Diet + Aquatic ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 116.92 ## 2 48 115.23 1 1.6878 0.7031 0.4059 5.3 Unbalanced ANOVA (advanced section/ optional) One-way and two-way ANOVA enabled us to determine the effect of categorical explanatory variables on a continuous response variable for the case of balanced experimental designs (i.e. when all levels of the explanatory variables contain the same number of replicates). However, loss of experimental units over the course of an experiment, or technical restriction of experimental designs can result in unbalanced designs. In this case, the above-mentioned ANOVA tests lead to misleading results related due to incorrect sum of squares calculations. For unbalanced experimental design, ANOVA must be modified to correctly account for the missing values of the response variable. A dataset is considered unbalanced when the sample sizes of two factor levels are not equal. The birdsdiet data is actually unbalanced (number of Aquatic and non-Aquatic is not equal) table(bird$Aquatic) ## ## 0 1 ## 39 15 While mathematical model, statistical hypothesis and statistical assumptions for ANOVA with unbalanced designs remain the same as for ANOVA with balanced designs, the sum of squares calculation changes. For unbalanced design, ANOVA thus test the hypothesis: H0: µ1 = µ2 =... = µi =... = µn H1: there is at least one µi that differs from the others. Using this mathematical model: \\(y_{ijk} = µ + A_{i} + B_{j} + A_{i}B_{j} + ε_{ijk}\\) Recall the sum of squares calculation of the ANOVA with balanced design: \\[ SS_{A} = rb\\sum_{i}(\\overline{y}_{i.}-\\overline{y})^2 = SS(A)\\] \\[ SS_{B} = ra \\sum_{j}(\\overline{y}_{.j}-\\overline{y})^2 = SS(B\\|A) = SS(A,B)-SS(B)\\] \\[ SS_{AB} = r \\sum{i,j,k}(\\overline{y}_{..k}-\\overline{y}_{.jk}-\\overline{y}_{i.k})^2= SS(A,B,AB)-SS(A,B)\\] This corresponds to sequential sum of squares, or Type I sum of squares, as the main effect of B is calculated after removing the main effect of A, and the interaction effect is calculated after removing the two main effects. Type I sum of suqares is the default type used in R. These calculations are sample size dependent as the effect of each factor is calculated after removing the effect of the precedent factor. For unbalanced design, ANOVA results will depend on the order in which each explanatory variable appears in the model. This can be seen by comparing the results of the following two models: unb_anov1 &lt;- lm(logMaxAbund ~ Aquatic + Diet, data = bird) unb_anov2 &lt;- lm(logMaxAbund ~ Diet + Aquatic, data = bird) anova(unb_anov1) ## Analysis of Variance Table ## ## Response: logMaxAbund ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Aquatic 1 1.228 1.2278 0.5114 0.47798 ## Diet 4 27.531 6.8827 2.8671 0.03291 * ## Residuals 48 115.230 2.4006 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(unb_anov2) ## Analysis of Variance Table ## ## Response: logMaxAbund ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Diet 4 27.071 6.7677 2.8191 0.03517 * ## Aquatic 1 1.688 1.6878 0.7031 0.40591 ## Residuals 48 115.230 2.4006 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 While the same explanatory variables are used in these two models, the ANOVA tables show different results due to the unbalanced design (i.e. different number of observations for aquatic and non-aquatic birds). A Type II sum of squares will test for the presence of a main effect after the other main effect. For unbalanced designs, marginal sum of squares, or Type III sum of squares is the most used. It performs calculations of main effect after removing the effect of all other factors ensure independence from sample size effects. SIf you are considering using Type II or III for your own dataset, you should read more about the subject. You can start with this link The equation for the sum of squre Type III is: \\[SS_{A}=SS(A\\|B,AB)=SS(A,B,AB)-SS(B,AB)\\] \\[SS_{B}=SS(B\\|A,AB)=SS(A,B,AB)-SS(A,AB)\\] \\[SS_{AB}=SS(AB\\|B,A)=SS(A,B,AB)-SS(B,AB)\\] In R, ANOVA with type III sum of squares can be implemented using the Anova function of package car and specifying “III” in the type option, for example: library(car) Anova(unb_anov1, type = &quot;III&quot;) Anova(unb_anov2, type = &quot;III&quot;) By comparing ANOVA tables of models with different order of explanatory variables, users can now note that the ANOVA table results always remain the same. Type III sum of squares thus correctly calculates the ANOVA results by becoming independent of sample sizes. After having verifying the model assumptions, results can finally be safely interpreted. "],["analysis-of-covariance-ancova.html", "Chapter 6 Analysis of covariance (ANCOVA)", " Chapter 6 Analysis of covariance (ANCOVA) Analysis of covariance (ANCOVA) is a linear model that tests the influence of one categorical explanatory variable (or more) and one continuous explanatory variable (or more) on a continuous response variable. Each level of the categorical variable is described by its own slope and intercept. In addition to testing if the response variable differs for at least one level of the categorical variable, ANCOVA also tests whether the response variable might be influenced by its relationship with the continuous variable (called the covariate in ANCOVA), and by any differences between group levels in the way that the continuous variable influences the response (i.e. the interaction). The ANCOVA hypotheses are thus: that there is no difference in the mean among levels of the categorical variable; there is no correlation between the response variable and the continuous explanatory variable; there is no interaction between the categorical and continuous explanatory variables. \\(Y = X * Z\\) where: \\(Y\\): Response variable is continuous \\(X\\): Explanatory variable is categorical \\(Z\\): Explanatory cariable is continuous \\[Y = \\mu + Main Effect Factors + Interaction between Factors + Main Effect Covariates + Interactions between Covariates Factors + \\epsilon\\] 6.0.1 6.1 Assumptions As with models seen above, to be valid ANCOVA models must meet the statistical assumptions of linear models that can be verified using diagnostic plots. In addition, ANCOVA models must have: The same value range for all covariates Variables that are fixed No interaction between categorical and continuous variables (not colinear) Note: A fixed variable is one that you are specifically interested in (i.e. bird mass). In contrast, a random variable is noise that you want to control for (i.e. site a bird was sampled in). If you have random variables, see the workshop on Linear And Generalized Linear Mixed Models! 6.0.2 6.2 Types of ANCOVA You can have any number of factors and/or covariates, but as their number increases, the interpretation of results gets more complex. The most frequently used ANCOVAs are those with: one covariate and one factor one covariate and two factors two covariates and one factor The different possible goals of the ANCOVA are to determine the effects of: the categorical and continuous variables on the response variable the categorical variable(s) on the response variable(s) after removing the effect of the continuous variable the categorical variable(s) on the relationship between the continuous variables(s) and the response variable Importantly, these goals are only met if there is no significant interaction between the categorical and continuous variables! Examples of significant interactions between the categorical and continuous variables (for an ANCOVA with one factor and one covariate) are illustrated by the second and third panels below: If the interaction is significant, you will have a scenario that looks like the left and middle graphs. If your covariate and factor are significant, outputs will look like the left graph. The same logic follows for ANCOVAs with multiple categorical and/or continuous variables. 6.0.3 6.3 Running an ANCOVA Running an ANCOVA in R is comparable to running a two-way ANOVA, using the function lm. However, instead of using two categorical variables (Diet and Aquatic), we now use one categorical and one continuous variable. For example, using a build in dataset called CO2, where the response variable is uptake, the continuous variable is conc and the factor is Treatment, the ANCOVA is: ancova.example &lt;- lm(uptake ~ conc * Treatment, data = CO2) anova(ancova.example) ## Analysis of Variance Table ## ## Response: uptake ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## conc 1 2285.0 2284.99 28.5535 8.377e-07 *** ## Treatment 1 988.1 988.11 12.3476 0.0007297 *** ## conc:Treatment 1 31.9 31.87 0.3983 0.5297890 ## Residuals 80 6402.0 80.02 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If only your categorical variable is significant, drop your continuous variable from the model: you will then have an ANOVA. If only your continuous variable is significant, drop your categorical variable from the model, you will then have a simple linear regression. If your interaction is significant, you might want to test which levels of your categorical variables ha(s)ve different slopes and to question whether ANCOVA is the most appropriate model. In the CO2 example above, both the continuous and categorical variable are significant, but the interaction is non-significant. If your replace Treatment with Type, however, you will see an example of a significant interaction. To compare the mean values of each factor, conditional on the effect of the other The effects::effect() function uses the output of the ANCOVA model to estimate the means of each factor level, corrected by the effect of the covariate install.packages(&quot;effects&quot;) library(effects) adj.means.ex &lt;- effect(&quot;Treatment&quot;, ancova.example) plot(adj.means.ex) CHALLENGE 4 Run an ANCOVA to test the effect of Diet, Mass, and their interaction on MaxAbund. Response variable: MaxAbund Explanatory variables: -Diet (factor with 5 levels) -Mass (numeric, continuous) str(bird) ## &#39;data.frame&#39;: 54 obs. of 9 variables: ## $ Family : chr &quot;Hawks&amp;Eagles&amp;Kites&quot; &quot;Long-tailed tits&quot; &quot;Larks&quot; &quot;Kingfishers&quot; ... ## $ MaxAbund : num 2.99 37.8 241.4 4.4 4.53 ... ## $ AvgAbund : num 0.674 4.04 23.105 0.595 2.963 ... ## $ Mass : num 716 5.3 35.8 119.4 315.5 ... ## $ Diet : Factor w/ 5 levels &quot;Insect&quot;,&quot;InsectVert&quot;,..: 5 1 4 5 2 4 5 1 1 5 ... ## $ Passerine : int 0 1 1 0 0 0 0 0 0 0 ... ## $ Aquatic : int 0 0 0 0 1 1 1 0 1 1 ... ## $ logMass : num 6.57 1.67 3.58 4.78 5.75 ... ## $ logMaxAbund: num 1.09 3.63 5.49 1.48 1.51 ... Cliquez pour voir la solution au Défi 3! # If you did the section on Contrasts above, you will need # to reset the contrast to Treatment for ease of comparison # using the &#39;&#39;options()&#39;&#39; function Otherwise, skip the # first line of code below options(contrasts = c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;)) # solution ancov1 &lt;- lm(logMaxAbund ~ logMass * Diet, data = bird) anova(ancov1) ## Analysis of Variance Table ## ## Response: logMaxAbund ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## logMass 1 10.464 10.4637 4.6054 0.03743 * ## Diet 4 17.749 4.4372 1.9530 0.11850 ## logMass:Diet 4 15.805 3.9513 1.7391 0.15849 ## Residuals 44 99.971 2.2721 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(ancov1) ## ## Call: ## lm(formula = logMaxAbund ~ logMass * Diet, data = bird) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.1308 -0.7081 -0.0855 0.9854 2.5694 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.7384 0.8042 4.648 3.05e-05 *** ## logMass -0.2865 0.1934 -1.481 0.1457 ## DietInsectVert -4.4599 6.1652 -0.723 0.4733 ## DietPlant 1.6796 4.7819 0.351 0.7271 ## DietPlantInsect 0.5537 1.3860 0.400 0.6914 ## DietVertebrate -7.7409 2.9359 -2.637 0.0115 * ## logMass:DietInsectVert 0.6745 1.2463 0.541 0.5911 ## logMass:DietPlant -0.2631 1.1602 -0.227 0.8216 ## logMass:DietPlantInsect 0.1115 0.3427 0.325 0.7465 ## logMass:DietVertebrate 1.1665 0.4556 2.561 0.0140 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.507 on 44 degrees of freedom ## Multiple R-squared: 0.3057, Adjusted R-squared: 0.1637 ## F-statistic: 2.153 on 9 and 44 DF, p-value: 0.04469 In this case, the interaction term is not significant, meaning that the effect of logMass on logMaxAbund does not differ between Diet groups. The interaction term can thus be dropped, and the ANCOVA model becomes: ancov2 &lt;- lm(logMaxAbund ~ logMass + Diet, data = bird) Because the R output shows that diet is also non-significant, this term is dropped, and the final model corresponds to: lm2 &lt;- lm(logMaxAbund ~ logMass, data = bird) The model results can also be graphically represented using a plot of the response variable as a function of the continuous explanatory variable with different point and line colours for the different levels of the categorical variable. Plot the ANCOVA intercept and slopes (model ancov1 above) using the abline() and coef() functions. coef(ancov1) ## (Intercept) logMass DietInsectVert ## 3.7384247 -0.2864726 -4.4598537 ## DietPlant DietPlantInsect DietVertebrate ## 1.6795975 0.5537085 -7.7409274 ## logMass:DietInsectVert logMass:DietPlant logMass:DietPlantInsect ## 0.6744893 -0.2631444 0.1114568 ## logMass:DietVertebrate ## 1.1665348 plot(logMaxAbund ~ logMass, data = bird, col = Diet, pch = 19, ylab = expression(&quot;log&quot;[10] * &quot;(Maximum Abundance)&quot;), xlab = expression(&quot;log&quot;[10] * &quot;(Mass)&quot;)) abline(a = coef(ancov1)[1], b = coef(ancov1)[2], col = &quot;deepskyblue1&quot;) abline(a = sum(coef(ancov1)[1] + coef(ancov1)[3]), b = sum(coef(ancov1)[2] + coef(ancov1)[7]), col = &quot;green2&quot;, lwd = 2) abline(a = sum(coef(ancov1)[1] + coef(ancov1)[4]), b = sum(coef(ancov1)[2] + coef(ancov1)[8]), col = &quot;orange1&quot;, lwd = 2) abline(a = sum(coef(ancov1)[1] + coef(ancov1)[5]), b = sum(coef(ancov1)[2] + coef(ancov1)[9]), col = &quot;lightsteelblue1&quot;, lwd = 2) abline(a = sum(coef(ancov1)[1] + coef(ancov1)[6]), b = sum(coef(ancov1)[2] + coef(ancov1)[10]), col = &quot;darkcyan&quot;, lwd = 2) "],["multiple-linear-regression.html", "Chapter 7 Multiple linear regression 7.1 Assumptions 7.2 Multiple linear regression in R 7.3 Polynomial regression (additional material) 7.4 Variation Partitioning (additional material)", " Chapter 7 Multiple linear regression A multiple regression tests the effects of several continuous explanatory variables on a continuous response variable. It differs from simple linear regression by having more than one explanatory variable. 7.0.1 Model formulation Variables The multiple linear regression is defined by the variables \\(y\\) representing the response variable (continuous) and \\(x\\) for the explanatory variables (continuous or categorical). The assumed relationship The relationship between the response variable and the predictors is defined in the same way as for simple regression. The difference is in the addition of \\(\\beta\\) parameters for the additional variables: \\[y_i = \\beta_0 + \\beta_1x_{1,i}+ \\beta_2x_{2,i}+ \\beta_3x_{3,i}+...+ \\beta_kx_{k,i} + \\epsilon_i\\] The parameter \\(\\beta_0\\) is the intercept (or constant) The parameter \\(\\beta_1\\) quantifies the effect of \\(x\\) on \\(y\\) The residual \\(\\epsilon_i\\) represents the unexplained variation The predicted value of \\(y_i\\) is defined as: \\(\\hat{y}_i = \\beta_0 + \\beta_1x_{1,i}+ \\beta_2x_{2,i}+ \\beta_3x_{3,i}+...+\\beta_kx_{k,i}\\). The unexplained variation or error remains normally distributed, centered on zero with a variance of \\(\\sigma^2\\) : \\[epsilon_i \\sim \\mathcal{N}(0,\\,\\sigma^2)\\] 7.1 Assumptions In the case of multiple linear regressions, two conditions are added to the usual conditions for linear models. First, there must be a linear relationship between each explanatory variable and the response variable. Second, the explanatory variables are independent of each other (there is no colinearity). 7.1.1 If variables are collinear In case of collinearity, there are some solutions: Keep only one of the variables collinear Try a multidimensional analysis (see Workshop 9) Try a pseudo-orthogonal analysis 7.2 Multiple linear regression in R 7.2.1 The data Using the Dickcissel dataset we will compare the relative importance of climate (clTma), productivity (NDVI) and soil cover (grass) as predictors of dickcissel abundance (abund). Dickcissel = read.csv(&quot;data/dickcissel.csv&quot;) str(Dickcissel) ## &#39;data.frame&#39;: 646 obs. of 15 variables: ## $ abund : num 5 0.2 0.4 0 0 0 0 0 0 0 ... ## $ Present : chr &quot;Absent&quot; &quot;Absent&quot; &quot;Absent&quot; &quot;Present&quot; ... ## $ clDD : num 5543 5750 5395 5920 6152 ... ## $ clFD : num 83.5 67.5 79.5 66.7 57.6 59.2 59.5 51.5 47.4 46.3 ... ## $ clTmi : num 9 9.6 8.6 11.9 11.6 10.8 10.8 11.6 13.6 13.5 ... ## $ clTma : num 32.1 31.4 30.9 31.9 32.4 32.1 32.3 33 33.5 33.4 ... ## $ clTmn : num 15.2 15.7 14.8 16.2 16.8 ... ## $ clP : num 140 147 148 143 141 ... ## $ NDVI : int -56 -44 -36 -49 -42 -49 -48 -50 -64 -58 ... ## $ broadleaf: num 0.3866 0.9516 0.9905 0.0506 0.2296 ... ## $ conif : num 0.0128 0.0484 0 0.9146 0.7013 ... ## $ grass : num 0 0 0 0 0 0 0 0 0 0 ... ## $ crop : num 0.2716 0 0 0.0285 0.044 ... ## $ urban : num 0.2396 0 0 0 0.0157 ... ## $ wetland : num 0 0 0 0 0 0 0 0 0 0 ... 7.2.2 Verify assumptions First, we must verify the presence of colinearity between all the explanatory and interest variables: An observable pattern between two explanatory variables may indicate that they are colinear! You must avoid this, or their effects on the response variable will be confounded. 7.2.3 Linear regression Now, let’s run the multiple regression of abundance (abund) against the variables clTma + NDVI + grass : # Multiple regression lm.mult &lt;- lm(abund ~ clTma + NDVI + grass, data = Dickcissel) summary(lm.mult) ## ## Call: ## lm(formula = abund ~ clTma + NDVI + grass, data = Dickcissel) ## ## Residuals: ## Min 1Q Median 3Q Max ## -35.327 -11.029 -4.337 2.150 180.725 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -83.60813 11.57745 -7.222 1.46e-12 *** ## clTma 3.27299 0.40677 8.046 4.14e-15 *** ## NDVI 0.13716 0.05486 2.500 0.0127 * ## grass 10.41435 4.68962 2.221 0.0267 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 22.58 on 642 degrees of freedom ## Multiple R-squared: 0.117, Adjusted R-squared: 0.1128 ## F-statistic: 28.35 on 3 and 642 DF, p-value: &lt; 2.2e-16 Then, let us check the other assumptions, as for the simple linear regression: # Assumptions par(mfrow = c(2, 2), mar = c(3.9, 4, 1.2, 1.1), oma = c(0, 0, 0, 0)) plot(lm.mult) 7.2.4 Find the best-fit model There is a principle of primary importance in model selection. It is the principle of parsimony. That is, explain the most variation with the least number of terms. We could therefore remove the least significant variable. summary(lm.mult)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -83.6081274 11.5774529 -7.221634 1.458749e-12 ## clTma 3.2729872 0.4067706 8.046272 4.135118e-15 ## NDVI 0.1371634 0.0548603 2.500231 1.265953e-02 ## grass 10.4143451 4.6896157 2.220725 2.671787e-02 All 3 variables are important. We keep everything! The model explains 11.28% of the variability in dickcissel abundance \\(R²_{adj} = 0.11\\). However, this information is not valid, because the conditions for applying the linear model are not met. It is important to note that the response variable does not vary linearly with the explanatory variables: par(mfrow = c(1, 3), mar = c(4, 4, 0.5, 0.5), cex = 1) plot(abund ~ clTma, data = Dickcissel) plot(abund ~ NDVI, data = Dickcissel) plot(abund ~ grass, data = Dickcissel) 7.3 Polynomial regression (additional material) As we noticed in the section on multiple linear regression, abund was non-linearly related to some variables To test for non-linear relationships, polynomial models of different degrees are compared. A polynomial model looks like this: \\[\\underbrace{2x^4}+\\underbrace{3x}-\\underbrace{2}\\] This polynomial has 3 terms. For a polynomial with one variable (\\(x\\)), the degree is the largest exponent of that variable. This is a the degree 4 polynomial: \\[2x^\\overbrace{4} + 3x - 2\\] When you know a degree, you can also give it a name: Degree Name Example 0 Constant \\(3\\) 1 Linear \\(x+9\\) 2 Quadratic \\(x^2-x+4\\) 3 Cubic \\(x^3-x^2+5\\) 4 Quartic \\(6x^4-x^3+x-2\\) 5 Quintic \\(x^5-3x^3+x^2+8\\) Now we can fix our problem with the Dickcissel dataset by testing the non-linear relationship between max abundance and temperature by comparing three sets of nested polynomial models (of degrees 0, 1, and 3): lm.linear &lt;- lm(abund ~ clDD, data = Dickcissel) lm.quad &lt;- lm(abund ~ clDD + I(clDD^2), data = Dickcissel) lm.cubic &lt;- lm(abund ~ clDD + I(clDD^2) + I(clDD^3), data = Dickcissel) By comparing the polynomial models and determine which nested model we should keep: summary(lm.linear) ## ## Call: ## lm(formula = abund ~ clDD, data = Dickcissel) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.062 -10.608 -7.758 -2.487 193.128 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.864566 2.757554 0.676 0.49918 ## clDD 0.001870 0.000588 3.180 0.00154 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.81 on 644 degrees of freedom ## Multiple R-squared: 0.01546, Adjusted R-squared: 0.01393 ## F-statistic: 10.11 on 1 and 644 DF, p-value: 0.001545 summary(lm.quad) ## ## Call: ## lm(formula = abund ~ clDD + I(clDD^2), data = Dickcissel) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.057 -12.253 -8.674 1.495 190.129 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.968e+01 5.954e+00 -3.306 0.001 ** ## clDD 1.297e-02 2.788e-03 4.651 4.00e-06 *** ## I(clDD^2) -1.246e-06 3.061e-07 -4.070 5.28e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 23.53 on 643 degrees of freedom ## Multiple R-squared: 0.04018, Adjusted R-squared: 0.0372 ## F-statistic: 13.46 on 2 and 643 DF, p-value: 1.876e-06 summary(lm.cubic) ## ## Call: ## lm(formula = abund ~ clDD + I(clDD^2) + I(clDD^3), data = Dickcissel) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.417 -12.247 -8.394 1.473 189.955 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.465e+01 1.206e+01 -1.215 0.225 ## clDD 8.612e-03 9.493e-03 0.907 0.365 ## I(clDD^2) -1.628e-07 2.277e-06 -0.071 0.943 ## I(clDD^3) -8.063e-11 1.680e-10 -0.480 0.631 ## ## Residual standard error: 23.54 on 642 degrees of freedom ## Multiple R-squared: 0.04053, Adjusted R-squared: 0.03605 ## F-statistic: 9.04 on 3 and 642 DF, p-value: 7.202e-06 Which one should you keep? 7.4 Variation Partitioning (additional material) Some of the selected explanatory variables in the multiple linear regression section were highly correlated./ Collinearity between explanatory variables can be assessed using the variance inflation factor vif() function of package car. Variable with VIF &gt; 5 are considered collinearity. mod &lt;- lm(clDD ~ clFD + clTmi + clTma + clP + grass, data = Dickcissel) car::vif(mod) ## clFD clTmi clTma clP grass ## 13.605855 9.566169 4.811837 3.196599 1.165775 In this example, clDD is correlated with clFD,clTmi and clTma. Instead of removing variable from the modal, we can reduce effect of colinearity by grouoping variables together. You can use varpart() to partition the variation in max abundance with all land cover variables (\"broadleaf\",\"conif\",\"grass\",\"crop\", \"urban\",\"wetland\") in one set and all climate variables in the other set (\"clDD\",\"clFD\",\"clTmi\",\"clTma\",\"clP\"). We can leave out NDVI for now. library(vegan) part.lm = varpart(Dickcissel$abund, Dickcissel[, c(&quot;clDD&quot;, &quot;clFD&quot;, &quot;clTmi&quot;, &quot;clTma&quot;, &quot;clP&quot;)], Dickcissel[, c(&quot;broadleaf&quot;, &quot;conif&quot;, &quot;grass&quot;, &quot;crop&quot;, &quot;urban&quot;, &quot;wetland&quot;)]) part.lm ## ## Partition of variance in RDA ## ## Call: varpart(Y = Dickcissel$abund, X = Dickcissel[, c(&quot;clDD&quot;, &quot;clFD&quot;, ## &quot;clTmi&quot;, &quot;clTma&quot;, &quot;clP&quot;)], Dickcissel[, c(&quot;broadleaf&quot;, &quot;conif&quot;, ## &quot;grass&quot;, &quot;crop&quot;, &quot;urban&quot;, &quot;wetland&quot;)]) ## ## Explanatory tables: ## X1: Dickcissel[, c(&quot;clDD&quot;, &quot;clFD&quot;, &quot;clTmi&quot;, &quot;clTma&quot;, &quot;clP&quot;)] ## X2: Dickcissel[, c(&quot;broadleaf&quot;, &quot;conif&quot;, &quot;grass&quot;, &quot;crop&quot;, &quot;urban&quot;, &quot;wetland&quot;)] ## ## No. of explanatory tables: 2 ## Total variation (SS): 370770 ## Variance: 574.84 ## No. of observations: 646 ## ## Partition table: ## Df R.squared Adj.R.squared Testable ## [a+b] = X1 5 0.31414 0.30878 TRUE ## [b+c] = X2 6 0.03654 0.02749 TRUE ## [a+b+c] = X1+X2 11 0.32378 0.31205 TRUE ## Individual fractions ## [a] = X1|X2 5 0.28456 TRUE ## [b] 0 0.02423 FALSE ## [c] = X2|X1 6 0.00327 TRUE ## [d] = Residuals 0.68795 FALSE ## --- ## Use function &#39;rda&#39; to test significance of fractions of interest Note: Collinear variables do not have to be removed prior to partitioning. With showvarpart(), we can visualise hpow these two groups (land cover and climate) explain variation in abund. For example: par(mar = rep(0.5, 4)) showvarparts(2) `?`(showvarparts) # With two explanatory tables, the fractions explained # uniquely by each of the two tables are ‘[a]’ and ‘[c]’, # and their joint effect is ‘[b]’ following Borcard et al. # (1992). Let’s try with our dataset Dickcissel and our model. plot(part.lm, digits = 2, bg = rgb(48, 225, 210, 80, maxColorValue = 225), col = &quot;turquoise4&quot;) Proportion of variance explained by: - Climate alone is 28.5% (given by X1|X2). - Land cover alone is ~0% (X2|X1). - Both combined is 2.4%. Unexplained variation by these groups (residuals) is 68.8%. We can now test the significance of each fraction: Climate out.1 = rda(Dickcissel$abund, Dickcissel[, c(&quot;clDD&quot;, &quot;clFD&quot;, &quot;clTmi&quot;, &quot;clTma&quot;, &quot;clP&quot;)], Dickcissel[, c(&quot;broadleaf&quot;, &quot;conif&quot;, &quot;grass&quot;, &quot;crop&quot;, &quot;urban&quot;, &quot;wetland&quot;)]) Land cover out.2 = rda(Dickcissel$abund, Dickcissel[, c(&quot;broadleaf&quot;, &quot;conif&quot;, &quot;grass&quot;, &quot;crop&quot;, &quot;urban&quot;, &quot;wetland&quot;)], Dickcissel[, c(&quot;clDD&quot;, &quot;clFD&quot;, &quot;clTmi&quot;, &quot;clTma&quot;, &quot;clP&quot;)]) # Climate anova(out.1, step = 1000, perm.max = 1000) ## Permutation test for rda under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(X = Dickcissel$abund, Y = Dickcissel[, c(&quot;clDD&quot;, &quot;clFD&quot;, &quot;clTmi&quot;, &quot;clTma&quot;, &quot;clP&quot;)], Z = Dickcissel[, c(&quot;broadleaf&quot;, &quot;conif&quot;, &quot;grass&quot;, &quot;crop&quot;, &quot;urban&quot;, &quot;wetland&quot;)]) ## Df Variance F Pr(&gt;F) ## Model 5 165.12 53.862 0.001 *** ## Residual 634 388.72 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Land cover anova(out.2, step = 1000, perm.max = 1000) ## Permutation test for rda under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(X = Dickcissel$abund, Y = Dickcissel[, c(&quot;broadleaf&quot;, &quot;conif&quot;, &quot;grass&quot;, &quot;crop&quot;, &quot;urban&quot;, &quot;wetland&quot;)], Z = Dickcissel[, c(&quot;clDD&quot;, &quot;clFD&quot;, &quot;clTmi&quot;, &quot;clTma&quot;, &quot;clP&quot;)]) ## Df Variance F Pr(&gt;F) ## Model 6 5.54 1.5063 0.167 ## Residual 634 388.72 The land cover fraction is non-significant once climate data is accounted for, which is not surprising given the low variation explained by the land cover. Thanks to variation partitioning, we were able to account for the collinearity of our variables and still test the effect of the climate and land cover in a simple and easy way! ANCOVA! But never forget to correctly specify your model and verify its statistical assumptions before interpreting its results according to the ecological background of your data. "],["summary.html", "Chapter 8 Summary", " Chapter 8 Summary The linear model describes the relationship between a response variable and one or more other predictor variables. It is used to analyze a well-formulated hypothesis, often associated with a more general research question. Regression determines whether variables are correlated by inferring the direction and strength of a relationship, and our confidence in the effect estimates. In the next workshop, you will learn how to program in R. You will learn how to use control structures (for, if, while loops) to avoid repeating code, to make organization easier, and to perform simulations. You will also learn how to write your own functions and some tricks to program more efficiently. "],["additional-resources.html", "Chapter 9 Additional resources", " Chapter 9 Additional resources 9.0.0.1 Cheat Sheets 9.0.0.2 A few books Myers RH - Classical and Modern Regression with Application Gotelli NJ - A Primer of Ecological Statistics 9.0.0.3 A few useful links "],["references.html", "Chapter 10 References", " Chapter 10 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
